# -*- coding: utf-8-sig -*-
import json
import os

import gradio as gr
import openai
import requests
from utils.chatgpt_utils import *

# è¨­ç½®æ‚¨çš„OpenAI APIé‡‘é‘°
openai.api_key = os.getenv("OPENAI_API_KEY")
URL = "https://api.openai.com/v1/chat/completions"

def index2context(idx:int):
    if idx is None or idx==0:
        return '[@PROMPT]'
    elif idx==1:
        return '[@GLOBAL]'
    elif idx == 2:
        return '[@SKIP]'
    elif idx == 3:
        return '[@SANDBOX]'
    elif idx == 4:
        return '[@EXPLAIN]'
    elif idx == 5:
        return '[@OVERRIDE]'
    else:
        return '[@PROMPT]'

def prompt_api(inputs, context_type, top_p, temperature, top_k, frequency_penalty, history=[]):
    _context_type=index2context(context_type)
    headers = {
        'Accept': 'text/event-stream',
        "Content-Type": "application/json",
        "Authorization": f"Bearer {openai.api_key}"
    }
    is_pause=False
    if _context_type == "[@GLOBAL]":
        history[0]["content"]= history[0]["content"]+'/n'+inputs

        chat = [(process_chat(history[i]), process_chat(history[i + 1])) for i in
                range(1, len(history) - 1, 2) if history[i]['role'] != 'system']

        yield chat, history,history
    elif _context_type == "[@OVERRIDE]":
        history[0]["content"]= inputs
        chat = [(process_chat(history[i]), process_chat(history[i + 1])) for i in
                range(1, len(history) - 1, 2) if history[i]['role'] != 'system']
        yield chat, history,history


    elif inputs:
        # èª¿ç”¨openai.ChatCompletion.createä¾†ç”Ÿæˆæ©Ÿå™¨äººçš„å›ç­”
        message_context = process_context(inputs, _context_type,history)
        partial_words = ''
        token_counter = 0
        payload = {
            "model": "gpt-3.5-turbo",
            "messages": message_context,
            "temperature": temperature,
            "top_p": top_p,
            "n": top_k,
            "stream": True,
            "frequency_penalty": frequency_penalty
        }


        history.append({"role": "user", "content": inputs,"context_type":_context_type})
        request = requests.post(URL, headers=headers, json=payload, stream=True)

        finish_reason = 'None'
        # client = sseclient.SSEClient(request)
        for chunk in request.iter_content(chunk_size=512, decode_unicode=False):

            try:
                if is_pause:
                    finish_reason = '[DONE]'
                    request.close()
                if chunk.decode('utf-8-sig').endswith('data: [DONE]\n\n'):
                    finish_reason = '[DONE]'
                    # sys.stdout.write('[DONE]')
                    break
                jstrs = chunk.decode('utf-8-sig').replace(':null', ':\"None\"')[5:]
                this_chunk = json.loads(jstrs)
                this_choice = this_chunk['choices'][0]['delta']
                finish_reason = this_chunk['choices'][0]['finish_reason']

                if 'content' in this_choice:
                    if partial_words == '' and this_choice['content'] == '\n\n':
                        pass
                    elif this_choice['content'] == '\n\n':
                        partial_words += '\n  '
                    else:
                        partial_words += this_choice['content']
                        # sys.stdout.write(this_choice['content'])
                    if token_counter == 0:
                        history.append({"role": "assistant", "content": partial_words,"context_type":_context_type})
                    else:
                        history[-1]['content'] = partial_words
                    chat = [(process_chat(history[i]), process_chat(history[i + 1])) for i in
                            range(1, len(history) - 1, 2) if history[i]['role'] != 'system']
                    token_counter += 1
                    yield chat, history,history



            except Exception as e:
                if len(partial_words) == 0:
                    pass
                else:
                    print(e)
                    print(pattern.findall(chunk.decode('utf-8-sig').replace(':null', ':\"None\"')))


        # æª¢æŸ¥finish_reasonæ˜¯å¦ç‚ºlength
        while finish_reason != '[DONE]':
            # è‡ªå‹•ä»¥userè§’è‰²ç™¼å‡ºã€Œç¹¼çºŒå¯«ä¸‹å»ã€çš„PROMPT
            # print("[ç¹¼çºŒå¯«ä¸‹å»]\n")
            prompt = "ç¹¼çºŒ"
            # èª¿ç”¨openai.ChatCompletion.createä¾†ç”Ÿæˆæ©Ÿå™¨äººçš„å›ç­”
            message_context = process_context(prompt,context_type, history)
            payload = {
                "model": "gpt-3.5-turbo",
                "messages": message_context,
                "temperature": temperature,
                "top_p": top_p,
                "n": top_k,
                "stream": True,
                "frequency_penalty": frequency_penalty
            }
            request = requests.post(URL, headers=headers, json=payload, stream=True)
            partial_words = ''
            finish_reason = 'None'
            # client = sseclient.SSEClient(request)
            for chunk in request.iter_content(chunk_size=512):
                # tt=chunk.decode('utf-8')[6:].rstrip('\n)
                try:

                    jstrs = chunk.decode('utf-8-sig').replace(':null', ':\"None\"')[5:]
                    this_chunk = json.loads(jstrs)
                    this_choice = this_chunk['choices'][0]['delta']
                    finish_reason = this_chunk['choices'][0]['finish_reason']
                    if chunk.decode('utf-8').endswith('data: [DONE]\n\n'):
                        finish_reason = '[DONE]'
                        sys.stdout.write('[DONE]')
                        break
                    if 'content' in this_choice:
                        if partial_words == '' and this_choice['content'] == '\n\n':
                            pass
                        elif this_choice['content'] == '\n\n':
                            partial_words += '\n'
                        else:
                            partial_words += this_choice['content']
                            # sys.stdout.write(this_choice['content'])
                        if token_counter == 0:
                            history.append({"role": "assistant", "content": partial_words,"context_type":_context_type})
                        else:
                            history[-1]['content'] = partial_words
                        chat = [(process_chat(history[i]), process_chat(history[i + 1])) for i in
                                range(1, len(history) - 1, 2) if history[i]['role'] != 'system']

                        token_counter += 1
                        yield chat, history,history

                except Exception as e:
                    if len(partial_words) == 0:
                        pass
                    else:
                        print(e)
                        print(chunk.decode('utf-8'))

        # æª¢æŸ¥æ¥çºŒå¾Œçš„å®Œæ•´å›è¦†æ˜¯å¦éé•·
        # print('bot_output: ',len(bot_output))

        if len(partial_words) > 500:
            # è‡ªå‹•ä»¥userè§’è‰²è¦æ±‚ã€Œå°‡æ­¤å…§å®¹é€²è¡Œç°¡çŸ­æ‘˜è¦ã€çš„promptéœ€
            print(len(partial_words))
            print('\n[ç°¡çŸ­æ‘˜è¦]\n')
            prompt = "ä½¿ç”¨ç¹é«”ä¸­æ–‡ç›´æ¥ä»¥ä¸‹å…§å®¹é€²è¡Œç°¡çŸ­æ‘˜è¦:" + partial_words
            # èª¿ç”¨openai.ChatCompletion.createä¾†ç”Ÿæˆæ©Ÿå™¨äººçš„å›ç­”

            payload = {
                "model": "gpt-3.5-turbo",
                "messages": prompt,
                "temperature": temperature,
                "top_p": top_p,
                "n": top_k,
                "stream": True,
                "frequency_penalty": frequency_penalty
            }
            request = requests.post(URL, headers=headers, json=payload, stream=True)
            summary = ''
            finish_reason = 'None'
            # client = sseclient.SSEClient(request)
            for chunk in request.iter_content(chunk_size=512):
                # tt=chunk.decode('utf-8')[6:].rstrip('\n)
                try:

                    if chunk.decode('utf-8').endswith('data: [DONE]\n\n'):
                        finish_reason = '[DONE]'
                        sys.stdout.write('[DONE]')
                        break

                    jstrs = chunk.decode('utf-8-sig').replace(':null', ':\"None\"')[5:]
                    this_chunk = json.loads(jstrs)
                    this_choice = this_chunk['choices'][0]['delta']
                    finish_reason = this_chunk['choices'][0]['finish_reason']

                    if 'content' in this_choice:
                        if summary == '' and this_choice['content'] == '\n\n':
                            pass
                        elif this_choice['content'] == '\n\n':
                            summary += '\n'
                        else:
                            summary += this_choice['content']
                            # sys.stdout.write(this_choice['content'])
                        history[-1]['summary'] = summary
                        chat = [(process_chat(history[i]), process_chat(history[i + 1])) for i in
                                range(1, len(history) - 1, 2) if history[i]['role'] != 'system']

                        token_counter += 1
                        yield chat, history,history
                except Exception as e:
                    if len(summary) == 0:
                        pass
                    else:
                        print(e)
                        print(chunk.decode('utf-8'))
            # messages_history.append({"role": "assistant", "content": summary})
            print('summary: ', len(summary),summary)
        else:
            chat = [(process_chat(history[i]), process_chat(history[i + 1])) for i in
                    range(1, len(history) - 1, 2) if history[i]['role'] != 'system']
            yield chat, history,history

        # é¡¯ç¤ºæ©Ÿå™¨äººçš„å›ç­”
        # window["bot_output"].update(bot_output)
        # å°‡ç”¨æˆ¶çš„è¼¸å…¥å’Œæ©Ÿå™¨äººçš„å›ç­”åŠ å…¥æ­·å²å°è©±è®Šæ•¸ä¸­

    else:
        # å¦‚æœç”¨æˆ¶æ²’æœ‰è¼¸å…¥ä»»ä½•å…§å®¹ï¼Œå‰‡æç¤ºç”¨æˆ¶
        yield "è«‹è¼¸å…¥æ‚¨æƒ³è¦èªªçš„è©±ã€‚", history,history

def nlu_api(text_input):
    # å‰µå»ºèˆ‡APIçš„å°è©±
    conversation = [
        {
            "role": "system",
            "content": "è«‹é€ä¸€è®€å–ä¸‹åˆ—å¥å­ï¼Œæ¯å€‹å¥å­å…ˆç†è§£èªæ„å¾Œå†é€²è¡Œåˆ†è©ã€æƒ…æ„Ÿåµæ¸¬ã€å‘½åå¯¦é«”åµæ¸¬ä»¥åŠæ„åœ–åµæ¸¬ã€‚åˆ†è©çµæœæ˜¯æŒ‡å°‡è¼¸å…¥çš„æ–‡å­—ï¼Œå…ˆé€²è¡Œèªæ„ç†è§£ï¼Œç„¶å¾ŒåŸºæ–¼èªæ„ç†è§£åˆç†æ€§çš„å‰æä¸‹ï¼Œå°‡è¼¸å…¥æ–‡å­—é€²è¡Œåˆ†è©(tokenize)ï¼Œè‹¥éå¿…è¦ï¼Œç›¡é‡ä¸è¦å‡ºç¾è¶…é3å€‹å­—çš„è©ï¼Œç„¶å¾Œä½¿ç”¨ã€Œ|ã€æ’å…¥è‡³åˆ†è©è©å½™å³æ§‹æˆåˆ†è©çµæœã€‚\néœ€è¦åµæ¸¬çš„æƒ…æ„Ÿé¡å‹\næ­£é¢æƒ…ç·’(positive_emotions):[è‡ªä¿¡,å¿«æ¨‚,é«”è²¼,å¹¸ç¦,ä¿¡ä»»,å–œæ„›,å°Šæ¦®,æœŸå¾…,æ„Ÿå‹•,æ„Ÿè¬,ç†±é–€,ç¨ç‰¹,ç¨±è®š]\nè² é¢æƒ…ç·’(negative_emotions):[å¤±æœ›,å±éšª,å¾Œæ‚”,å†·æ¼ ,æ‡·ç–‘,ææ‡¼,æ‚²å‚·,æ†¤æ€’,æ“”å¿ƒ,ç„¡å¥ˆ,ç…©æ‚¶,è™›å‡,è¨å­,è²¶è²¬,è¼•è¦–]\nç•¶å¥å­ä¸­æœ‰ç¬¦åˆä»¥ä¸Šä»»ä½•æƒ…æ„Ÿç¨®é¡æ™‚ï¼Œè«‹ç›¡å¯èƒ½çš„å°‡ç¬¦åˆçš„ã€Œæƒ…æ„Ÿç¨®é¡ã€åŠå¥å­ä¸­çš„é‚£äº›ã€Œè§¸åŠåˆ°æƒ…æ„Ÿç¨®é¡çš„å…§å®¹ã€æˆå°çš„åˆ—èˆ‰å‡ºä¾†ï¼Œä¸€å€‹å¥å­å¯ä»¥è§¸åŠä¸åªä¸€ç¨®æƒ…æ„Ÿã€‚\néœ€è¦åµæ¸¬çš„å¯¦é«”é¡å‹(entities)[ä¸­æ–‡äººå,ä¸­æ–‡ç¿»è­¯äººå,å¤–èªäººå,åœ°å/åœ°é»,æ™‚é–“,å…¬å¸æ©Ÿæ§‹å/å“ç‰Œå,å•†å“å,å•†å“è¦æ ¼,åŒ–åˆç‰©å/æˆåˆ†å,å…¶ä»–å°ˆæœ‰åè©,é‡‘é¡,å…¶ä»–æ•¸å€¼]\næ­¤å¤–ï¼Œè‹¥æ˜¯å¥å­ä¸­æœ‰åµæ¸¬åˆ°ç¬¦åˆä¸Šè¿°å¯¦é«”é¡å‹æ™‚ï¼Œä¹Ÿè«‹ç›¡å¯èƒ½çš„å°‡ç¬¦åˆçš„ã€Œå¯¦é«”é¡å‹ã€åŠå¥å­ä¸­çš„é‚£äº›ã€Œè§¸åŠåˆ°å¯¦é«”é¡å‹å…§å®¹ï½£æˆå°çš„åˆ—èˆ‰å‡ºä¾†ï¼Œä¸€å€‹å¥å­å¯ä»¥è§¸åŠä¸åªä¸€ç¨®å¯¦é«”é¡å‹ã€‚ç•¶ä½ åµæ¸¬åˆ°å¥å­ä¸­æœ‰è¦æ±‚ä½ ä»£ç‚ºåŸ·è¡ŒæŸå€‹ä»»å‹™æˆ–æ˜¯æŸ¥è©¢æŸè³‡è¨Šçš„æ„åœ–(intents)æ™‚ï¼Œæ ¹æ“šä»¥è‹±æ–‡ã€Œåè©+å‹•è©-ingã€çš„é§å³°å¼å‘½åå½¢å¼ä¾†çµ„æˆæ„åœ–é¡åˆ¥(ä¾‹å¦‚ä½¿ç”¨è€…èªªã€Œè«‹å¹«æˆ‘è¨‚ä»Šå¤©ä¸‹åˆ5é»å»é«˜é›„çš„ç«è»Šç¥¨ã€å…¶æ„åœ–é¡åˆ¥ç‚ºTicketOrdering)ï¼ŒåŠå¥å­ä¸­çš„é‚£äº›ã€Œè§¸åŠåˆ°æ„åœ–é¡åˆ¥çš„å…§å®¹ã€æˆå°çš„åˆ—èˆ‰å‡ºä¾†ï¼Œä¸€å€‹å¥å­å¯ä»¥è§¸åŠä¸åªä¸€ç¨®æ„åœ–ã€‚ä»¥ä¸‹ç‚ºã€Œå¼µå¤§å¸¥çš„äººç”Ÿæ˜¯ä¸€å¼µèŒ¶å‡ ï¼Œä¸Šé¢æ”¾æ»¿äº†æ¯å…·ã€‚è€Œæœ¬èº«å°±æ˜¯æ¯å…·ã€çš„ç¯„ä¾‹è§£æçµæœ\n"
            "{\nsentence:  \"å¼µå¤§å¸¥çš„äººç”Ÿæ˜¯ä¸€å¼µèŒ¶å‡ ï¼Œä¸Šé¢æ”¾æ»¿äº†æ¯å…·ã€‚è€Œæœ¬èº«å°±æ˜¯æ¯å…·\",\nsegmented_sentence:  \"å¼µå¤§å¸¥|çš„|äººç”Ÿ|æ˜¯|ä¸€|å¼µ|èŒ¶å‡ |ï¼Œ|ä¸Šé¢|æ”¾æ»¿äº†|æ¯å…·|ã€‚|è€Œ|æœ¬èº«|å°±æ˜¯|æ¯å…·\",\npositive_emotions:  [\n0:  {\ntype:  \"ç…©æ‚¶\",\ncontent:  \"æ”¾æ»¿äº†æ¯å…·\"\n} ,\n1:  {\ntype:  \"ç„¡å¥ˆ\",\ncontent:  \"æœ¬èº«å°±æ˜¯æ¯å…·\"\n}\n],\nnegative_emotions:  [\n0:  {\ntype:  \"å¤±æœ›\",\ncontent:  \"ä¸Šé¢æ”¾æ»¿äº†æ¯å…·\"\n} \n],\nentities:  [\n0:  {\ntype:  \"ä¸­æ–‡äººå\",\ncontent:\"å¼µå¤§å¸¥\"\n}\n]\n}\n\ræœ€å¾Œå°‡æ¯å€‹å¥å­çš„è§£æçµæœæ•´åˆæˆå–®ä¸€jsonæ ¼å¼ï¼Œç¸®é€²é‡ç‚º1ã€‚"
        },
        {
            "role": "user",
            "content": text_input
        }
    ]
    # å‘OpenAI ChatGPT APIç™¼é€è«‹æ±‚
    response = openai.ChatCompletion.create(
        model="gpt-3.5-turbo",
        messages=conversation,
        temperature=0.1
    )
    try:
        # è§£æè¿”å›çš„JSONçµæœ
        jstrs=pattern.findall(response.choices[0].message['content'].strip())
        jstrs=jstrs[0] if len(jstrs)==1 else '['+', '.join(jstrs)+']'
        output_json = json.loads(jstrs)
        return json.dumps(output_json, ensure_ascii=False, indent=4)
    except Exception as e:
        return response.choices[0].message['content'].strip()+"\n"+str(e)

def reset_textbox():
    return gr.update(value='')

def reset_context():
    return gr.update(value="[@PROMPT] ä¸€èˆ¬æŒ‡ä»¤")

def pause_message():
    is_pause=True



title = """<h1 align="center">ğŸ”¥ğŸ¤–ChatGPT Streaming ğŸš€</h1>"""
description = ""

with gr.Blocks(css="""#col_container {width: 70%; margin-left: auto; margin-right: auto;}
                #chatbot {height: 50%; overflow: auto;}
                #history_viewer {height: 50%; overflow: auto;}""",theme=gr.themes.Soft(spacing_size="sm", radius_size="none",font=["Microsoft JhengHei UI", "Arial", "sans-serif"])) as demo:
    gr.HTML(title)
    with gr.Tabs():
        with gr.TabItem("å°è©±"):
            with gr.Column(elem_id="col_container"):
                chatbot = gr.Chatbot(color_map=("orange", "dark gray"),elem_id='chatbot')  # c
                with gr.Row():
                    radio = gr.Radio(["å‰µæ„", "å¹³è¡¡", "ç²¾ç¢º"], show_label=False,interactive=True)
                    context_type = gr.Dropdown(
                        ["[@PROMPT] ä¸€èˆ¬æŒ‡ä»¤", "[@GLOBAL] å…¨å±€æŒ‡ä»¤", "[@SKIP] è·³è„«ä¸Šæ–‡", "[@SANDBOX] æ²™ç®±éš”çµ•", "[@EXPLAIN] è§£é‡‹ä¸Šæ–‡", "[@OVERRIDE] è¦†å¯«å…¨å±€"],
                        value="[@PROMPT] ä¸€èˆ¬æŒ‡ä»¤",type='index',label="contextè™•ç†", elem_id='context_type',interactive=True)
                    scenario_type = gr.Dropdown(
                        ["(ç„¡)","é›»å­éƒµä»¶", "éƒ¨è½æ ¼æ–‡ç« ","æ³•å¾‹æ–‡ä»¶"],value="(ç„¡)", label="æ‡‰ç”¨å ´æ™¯", elem_id='scenario',interactive=True)
                with gr.Row():
                    inputs = gr.Textbox(placeholder="ä½ èˆ‡èªè¨€æ¨¡å‹Bertæœ‰ä½•ä¸åŒ?", label="è¼¸å…¥æ–‡å­—å¾ŒæŒ‰enter")  # t
                with gr.Row():
                    b1 = gr.Button(value='é€å‡º')
                with gr.Row():
                    b3 = gr.Button(value='ğŸ—‘ï¸')
                    b2 = gr.Button(value='â¸ï¸')
                state = gr.State([{"role": "system", "content": 'æ‰€æœ‰å…§å®¹ä»¥ç¹é«”ä¸­æ–‡æ›¸å¯«'}])  # s
        with gr.TabItem("æ­·å²"):
            with gr.Column(elem_id="col_container"):
                history_viewer =gr.JSON(elem_id='history_viewer')
        with gr.TabItem("NLU"):
            with gr.Column(elem_id="col_container"):
                gr.Markdown("å°‡æ–‡æœ¬è¼¸å…¥åˆ°ä¸‹é¢çš„æ–¹å¡Šä¸­ï¼ŒæŒ‰ä¸‹ã€Œé€å‡ºã€æŒ‰éˆ•å°‡æ–‡æœ¬é€£åŒä¸Šè¿°çš„promptç™¼é€è‡³OpenAI ChatGPT APIï¼Œç„¶å¾Œå°‡è¿”å›çš„JSONé¡¯ç¤ºåœ¨è¦–è¦ºåŒ–ç•Œé¢ä¸Šã€‚")
                with gr.Row():
                    nlu_inputs = gr.Textbox(lines=6, placeholder="è¼¸å…¥å¥å­...")
                    nlu_output =gr.Text(label="å›å‚³çš„JSONè¦–è¦ºåŒ–",interactive=True,max_lines=40)
                nlu_button = gr.Button("é€å‡º")



        # with gr.TabItem("ç¿»è­¯"):
        #     with gr.Column(elem_id="col_container"):
        #         history_viewer = gr.Text(elem_id='history_viewer',max_lines=30)
    # inputs, top_p, temperature, top_k, repetition_penalty
    with gr.Accordion("è¶…åƒæ•¸", open=False):
        top_p = gr.Slider(minimum=-0, maximum=1.0, value=0.95, step=0.05, interactive=True,
                          label="é™åˆ¶å–æ¨£ç¯„åœ(Top-p)", )
        temperature = gr.Slider(minimum=-0, maximum=5.0, value=0.5, step=0.1, interactive=True,
                                label="æº«åº¦ (Temperature)", )
        top_k = gr.Slider(minimum=1, maximum=50, value=1, step=1, interactive=True, label="å€™é¸çµæœå€‹æ•¸(Top-k)", )
        frequency_penalty = gr.Slider(minimum=-2, maximum=2, value=0, step=0.01, interactive=True,
                                      label="é‡è¤‡æ€§è™•ç½°(Frequency Penalty)",
                                      info='å€¼åŸŸç‚º-2~+2ï¼Œæ•¸å€¼è¶Šå¤§ï¼Œå°æ–¼é‡è¤‡ç”¨å­—æœƒçµ¦äºˆæ‡²ç½°ï¼Œæ•¸å€¼è¶Šè² ï¼Œå‰‡é¼“å‹µé‡è¤‡ç”¨å­—')



    inputs.submit(prompt_api, [inputs, context_type, top_p, temperature, top_k, frequency_penalty, state],
                  [chatbot, state,history_viewer]).then(reset_context, [], [context_type]).then(reset_textbox, [], [inputs])
    b1.click(prompt_api, [inputs, context_type, top_p, temperature, top_k, frequency_penalty, state], [chatbot, state,history_viewer], )
    b3.click(reset_textbox, [], [inputs])
    b2.click(fn=pause_message, inputs=[], outputs=None)


    nlu_inputs.submit(nlu_api, [nlu_inputs],[nlu_output])
    nlu_button.click(nlu_api, [nlu_inputs], [nlu_output])

    gr.Markdown(description)
    demo.queue(concurrency_count=3,api_open=False).launch(debug=True)
