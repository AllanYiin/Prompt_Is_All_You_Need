# -*- coding: utf-8-sig -*-
import json
import os
import datetime
import time
import string
import builtins
import uuid

import regex
from pathlib import Path
import gradio as gr
import openai
import copy
import requests
import asyncio
import time
import nest_asyncio
import openai_async
import whisper
import math
import numpy as np
from pydub import AudioSegment
from datetime import datetime, timedelta
from queue import Queue

nest_asyncio.apply()
from collections import OrderedDict
from datetime import datetime
from prompt4all.utils.chatgpt_utils import *
from prompt4all.utils.regex_utils import *
import prompt4all.api.context_type as ContextType
from prompt4all.api.base_api import *
from prompt4all.api.message import *
from prompt4all.utils.tokens_utils import *
from prompt4all.utils.summary_utils import *
from prompt4all.utils.pdf_utils import *
from prompt4all.utils.io_utils import process_file
from prompt4all.theme import adjust_theme, advanced_css
from prompt4all.utils.whisper_utils import *
from prompt4all.common import *

from prompt4all import context
from prompt4all.context import *
from prompt4all.common import find_available_port
from prompt4all.ui import settings_ui
from prompt4all.ui import rewrite_ui
os.chdir(os.path.dirname(__file__))
cxt = context._context()
os.environ['no_proxy'] = '*'

# è¨­ç½®æ‚¨çš„OpenAI APIé‡‘é‘°
# è«‹å°‡æ‚¨çš„é‡‘é‘°å€¼å¯«å…¥è‡³ç’°å¢ƒè®Šæ•¸"OPENAI_API_KEY"ä¸­
# os.environ['OPENAI_API_KEY']=#'ä½ çš„é‡‘é‘°å€¼'
if "OPENAI_API_KEY" not in os.environ:
    print("OPENAI_API_KEY  is not exists!")

openai.api_key = os.getenv("OPENAI_API_KEY")
model_lists = openai.models.list()

##
initialize_conversation_history()


def index2context(idx: int):
    if idx is None or idx == 0:
        return ContextType.prompt
    elif idx == 1:
        return ContextType.globals
    elif idx == 2:
        return ContextType.skip
    elif idx == 3:
        return ContextType.sandbox
    elif idx == 4:
        return ContextType.explain
    elif idx == 5:
        return ContextType.override
    else:
        return '[@PROMPT]'


def prompt_api(inputs, context_type, top_p, temperature, top_k, frequency_penalty, full_history=[]):
    _context_type = index2context(context_type)
    baseChatGpt.API_PARAMETERS['temperature'] = temperature
    baseChatGpt.API_PARAMETERS['top_p'] = top_p
    baseChatGpt.API_PARAMETERS['top_k'] = top_k
    baseChatGpt.API_PARAMETERS['frequency_penalty'] = frequency_penalty
    if isinstance(full_history, gr.State):
        full_history=full_history.value
    streaming_chat = baseChatGpt.post_a_streaming_chat(inputs, _context_type, baseChatGpt.API_PARAMETERS, full_history)

    while True:
        if _context_type == ContextType.override:
            if len(cxt.conversation_history.selected_item.mapping) > 2:
                keys = list(cxt.conversation_history.selected_item.mapping.keys())
                cxt.conversation_history.selected_item.mapping[keys[1]].message.content.parts[0] = inputs
        elif _context_type == ContextType.globals:
            if len(cxt.conversation_history.selected_item.mapping) > 2:
                keys = list(cxt.conversation_history.selected_item.mapping.keys())
                cxt.conversation_history.selected_item.mapping[keys[1]].message.content.parts.append(inputs)
        else:
            _current_item=cxt.conversation_history.selected_item.current_item
            mid = str(uuid.uuid4())
            if _current_item.children is None:
                _current_item.children=[]
            _current_item.children.append(mid)
            cxt.conversation_history.selected_item.mapping[mid]=Mapping(mapping_id=mid,parent=_current_item.id).new_user_message_mapping(inputs)
            cxt.conversation_history.selected_item.current_node=mid

            pass
        try:
            chat, answer, full_history = next(streaming_chat)
            yield chat, full_history, full_history
        except StopIteration:
            break
        except Exception as e:
            raise gr.Error(str(e))


def nlu_api(text_input):
    # å‰µå»ºèˆ‡APIçš„å°è©±

    text_inputs = text_input.split('\n')
    _parameters = copy.deepcopy(baseChatGpt.API_PARAMETERS)
    _parameters['temperature'] = 0.1
    results = []
    for txt in text_inputs:
        conversation = [
            {
                "role": "system",
                "content": "è«‹é€ä¸€è®€å–ä¸‹åˆ—å¥å­ï¼Œæ¯å€‹å¥å­å…ˆç†è§£èªæ„å¾Œå†é€²è¡Œåˆ†è©ã€æƒ…æ„Ÿåµæ¸¬ã€å‘½åå¯¦é«”åµæ¸¬ä»¥åŠæ„åœ–åµæ¸¬ã€‚åˆ†è©çµæœæ˜¯æŒ‡å°‡è¼¸å…¥çš„æ–‡å­—ï¼Œå…ˆé€²è¡Œèªæ„ç†è§£ï¼Œç„¶å¾ŒåŸºæ–¼èªæ„ç†è§£åˆç†æ€§çš„å‰æä¸‹ï¼Œå°‡è¼¸å…¥æ–‡å­—é€²è¡Œåˆ†è©(tokenize)ï¼Œè‹¥éå¿…è¦ï¼Œç›¡é‡ä¸è¦å‡ºç¾è¶…é3å€‹å­—çš„è©ï¼Œç„¶å¾Œä½¿ç”¨ã€Œ|ã€æ’å…¥è‡³åˆ†è©è©å½™å³æ§‹æˆåˆ†è©çµæœã€‚\néœ€è¦åµæ¸¬çš„æƒ…æ„Ÿé¡å‹\næ­£é¢æƒ…ç·’(positive_emotions):[è‡ªä¿¡,å¿«æ¨‚,é«”è²¼,å¹¸ç¦,ä¿¡ä»»,å–œæ„›,å°Šæ¦®,æœŸå¾…,æ„Ÿå‹•,æ„Ÿè¬,ç†±é–€,ç¨ç‰¹,ç¨±è®š]\nè² é¢æƒ…ç·’(negative_emotions):[å¤±æœ›,å±éšª,å¾Œæ‚”,å†·æ¼ ,æ‡·ç–‘,ææ‡¼,æ‚²å‚·,æ†¤æ€’,æ“”å¿ƒ,ç„¡å¥ˆ,ç…©æ‚¶,è™›å‡,è¨å­,è²¶è²¬,è¼•è¦–]\nç•¶å¥å­ä¸­æœ‰ç¬¦åˆä»¥ä¸Šä»»ä½•æƒ…æ„Ÿç¨®é¡æ™‚ï¼Œè«‹ç›¡å¯èƒ½çš„å°‡ç¬¦åˆçš„ã€Œæƒ…æ„Ÿç¨®é¡ã€åŠå¥å­ä¸­çš„é‚£äº›ã€Œè§¸åŠåˆ°æƒ…æ„Ÿç¨®é¡çš„å…§å®¹ã€æˆå°çš„åˆ—èˆ‰å‡ºä¾†ï¼Œä¸€å€‹å¥å­å¯ä»¥è§¸åŠä¸åªä¸€ç¨®æƒ…æ„Ÿã€‚\néœ€è¦åµæ¸¬çš„å¯¦é«”é¡å‹(entities)æ‡‰è©²åŒ…æ‹¬ä½†ä¸åƒ…é™æ–¼[ä¸­æ–‡äººå,ä¸­æ–‡ç¿»è­¯äººå,å¤–èªäººå,æ­Œæ‰‹/æ¨‚åœ˜/åœ˜é«”åç¨±,åœ°å/åœ°é»,æ™‚é–“,å…¬å¸æ©Ÿæ§‹å/å“ç‰Œå,å•†å“å,å•†å“è¦æ ¼,åŒ–åˆç‰©å/æˆåˆ†å,æ­Œæ›²/æ›¸ç±/ä½œå“åç¨±,å…¶ä»–å°ˆæœ‰åè©,é‡‘é¡,å…¶ä»–æ•¸å€¼]ï¼Œä½ å¯ä»¥è¦–æƒ…æ³æ“´å……ï¼Œ\næ­¤å¤–ï¼Œè‹¥æ˜¯å¥å­ä¸­æœ‰åµæ¸¬åˆ°ç¬¦åˆä¸Šè¿°å¯¦é«”é¡å‹æ™‚ï¼Œä¹Ÿè«‹ç›¡å¯èƒ½çš„å°‡ç¬¦åˆçš„ã€Œå¯¦é«”é¡å‹ã€åŠå¥å­ä¸­çš„é‚£äº›ã€Œè§¸åŠåˆ°å¯¦é«”é¡å‹å…§å®¹ï½£æˆå°çš„åˆ—èˆ‰å‡ºä¾†ï¼Œä¸€å€‹å¥å­å¯ä»¥è§¸åŠä¸åªä¸€ç¨®å¯¦é«”é¡å‹ã€‚ç•¶ä½ åµæ¸¬åˆ°å¥å­ä¸­æœ‰è¦æ±‚ä½ ä»£ç‚ºåŸ·è¡ŒæŸå€‹ä»»å‹™ã€æˆ–æ˜¯è¡¨é”è‡ªå·±æƒ³è¦çš„äº‹ç‰©æˆ–æ˜¯è¡Œå‹•ã€æˆ–æ˜¯æƒ³æŸ¥è©¢æŸè³‡è¨Šçš„æ„åœ–(intents)æ™‚ï¼Œæ ¹æ“šä»¥æ„åœ–æœ€æ™®éçš„è‹±æ–‡è¬›æ³•ä¹‹ã€Œåè©+å‹•è©-ingã€çš„é§å³°å¼å‘½åå½¢å¼ä¾†çµ„æˆæ„åœ–é¡åˆ¥(ä¾‹å¦‚ä½¿ç”¨è€…èªªã€Œè«‹å¹«æˆ‘è¨‚ä»Šå¤©ä¸‹åˆ5é»å»é«˜é›„çš„ç«è»Šç¥¨ã€å…¶æ„åœ–é¡åˆ¥ç‚ºTicketOrdering)ï¼ŒåŠå¥å­ä¸­çš„é‚£äº›ã€Œè§¸åŠåˆ°æ„åœ–é¡åˆ¥çš„å…§å®¹ã€æˆå°çš„åˆ—èˆ‰å‡ºä¾†ï¼Œä¸€å€‹å¥å­å¯ä»¥è§¸åŠä¸åªä¸€ç¨®æ„åœ–ã€‚ä»¥ä¸‹ç‚ºã€Œå¼µå¤§å¸¥çš„äººç”Ÿæ˜¯ä¸€å¼µèŒ¶å‡ ï¼Œä¸Šé¢æ”¾æ»¿äº†æ¯å…·ã€‚è€Œæœ¬èº«å°±æ˜¯æ¯å…·ã€çš„ç¯„ä¾‹è§£æçµæœ\n"
                           "{\nsentence:  \"å¼µå¤§å¸¥çš„äººç”Ÿæ˜¯ä¸€å¼µèŒ¶å‡ ï¼Œä¸Šé¢æ”¾æ»¿äº†æ¯å…·ã€‚è€Œæœ¬èº«å°±æ˜¯æ¯å…·\",\nsegmented_sentence:  \"å¼µå¤§å¸¥|çš„|äººç”Ÿ|æ˜¯|ä¸€|å¼µ|èŒ¶å‡ |ï¼Œ|ä¸Šé¢|æ”¾æ»¿äº†|æ¯å…·|ã€‚|è€Œ|æœ¬èº«|å°±æ˜¯|æ¯å…·\",\npositive_emotions:  [\n0:  {\ntype:  \"ç…©æ‚¶\",\ncontent:  \"æ”¾æ»¿äº†æ¯å…·\"\n} ,\n1:  {\ntype:  \"ç„¡å¥ˆ\",\ncontent:  \"æœ¬èº«å°±æ˜¯æ¯å…·\"\n}\n],\nnegative_emotions:  [\n0:  {\ntype:  \"å¤±æœ›\",\ncontent:  \"ä¸Šé¢æ”¾æ»¿äº†æ¯å…·\"\n} \n],\nentities:  [\n0:  {\ntype:  \"ä¸­æ–‡äººå\",\ncontent:\"å¼µå¤§å¸¥\"\n}\n]\n}\n\ræœ€å¾Œå°‡æ¯å€‹å¥å­çš„è§£æçµæœæ•´åˆæˆå–®ä¸€jsonæ ¼å¼ï¼Œç¸®é€²é‡ç‚º1ã€‚"
            },
            {
                "role": "user",
                "content": txt
            }
        ]
        jstrs = json_pattern.findall(baseChatGpt.post_and_get_answer(conversation, _parameters))
        jstrs = jstrs[0] if len(jstrs) == 1 else '[' + ', '.join(jstrs) + ']'
        output_json = json.loads(jstrs)
        results.append(json.dumps(output_json, ensure_ascii=False, indent=3))

        yield '[' + ', '.join(results) + ']'


def image_api(text_input, image_size, temperature=1.2):
    # å‰µå»ºèˆ‡APIçš„å°è©±
    _system_prompt = open("prompts/dalle2.md", encoding="utf-8").read()
    _parameters = copy.deepcopy(baseChatGpt.API_PARAMETERS)
    _parameters['temperature'] = temperature
    _parameters['max_tokens'] = 100
    results = []
    conversation = [
        {
            "role": "system",
            "content": _system_prompt
        },
        {
            "role": "user",
            "content": text_input
        }
    ]
    image_prompt = imageChatGpt.post_and_get_answer(conversation, _parameters)
    if ':' in image_prompt:
        image_prompt = ' '.join(image_prompt.split(':')[1:])
    images_urls = imageChatGpt.generate_images(image_prompt, text_input, image_size)
    return image_prompt, images_urls





async def summarize_text(text_input, system_prompt):
    """post ä¸²æµå½¢å¼çš„å°è©±
    :param system_prompt:
    :param text_input:
    :return:
    """
    partial_words = ''
    token_counter = 0
    context_type = ContextType.skip
    passage = "è¼¸å…¥æ–‡å­—å…§å®¹:\"\"\"\n{0}\n\"\"\"\n".format(text_input)
    conversation = [
        {
            "role": "system",
            "content": system_prompt
        },
        {
            "role": "user",
            "content": passage
        }
    ]
    _parameters = copy.deepcopy(summaryChatGpt.API_PARAMETERS)
    _parameters['temperature'] = 0.001
    _parameters['presence_penalty'] = 1.2
    payload = summaryChatGpt.parameters2payload(summaryChatGpt.API_MODEL, conversation, _parameters, stream=False)

    response = await asyncio.to_thread(
        requests.post,
        summaryChatGpt.BASE_URL, headers=summaryChatGpt.API_HEADERS, json=payload, stream=False
    )

    try:
        # è§£æè¿”å›çš„JSONçµæœ
        this_choice = json.loads(response.content.decode())['choices'][0]
        print(this_choice)
        summary = this_choice["message"]
        total_tokens = response.json()["usage"]['completion_tokens']
        summary['total_tokens'] = total_tokens
        return summary
    except Exception as e:
        raise gr.Error(str(response.json()) + "\n" + str(e))


async def rolling_summary(large_inputs, full_history, summary_method, summary_options):
    _parameters = copy.deepcopy(summaryChatGpt.API_PARAMETERS)
    _parameters['temperature'] = 0
    _parameters['presence_penalty'] = 1.2
    large_inputs = large_inputs.split('\n') if isinstance(large_inputs, str) else large_inputs
    large_inputs_tokens = builtins.sum([estimate_used_tokens(w) + 1 for w in large_inputs])
    large_inputs_bk = copy.deepcopy(large_inputs)
    st = datetime.now()

    is_final_stage = False
    keep_summary = True
    summary_repository = OrderedDict()
    cleansed_summary = []
    mindmap_history = ""
    mindmap_head = '# æ‘˜è¦å¿ƒæ™ºåœ–'
    meeting_minutes = ''
    meeting_head = '# æœƒè­°è¨˜éŒ„'
    topic_shortcuts = ''
    topic_head = '# ä¸»é¡Œé‡é»'

    if summary_method == 0:
        _system_prompt = open("prompts/rolling_summary.md", encoding="utf-8").read()
        _final_prompt = open("prompts/summary_final_cleansing.md", encoding="utf-8").read()

        summary_history = 'ç©ºçš„æ¸…å–®'
        this_system_tokens = estimate_used_tokens(_system_prompt) + estimate_used_tokens('system',
                                                                                         model_name=summaryChatGpt.API_MODEL) + 4
        this_final_tokens = estimate_used_tokens(_final_prompt) + estimate_used_tokens('system',
                                                                                       model_name=summaryChatGpt.API_MODEL) + 4
        available_tokens = summaryChatGpt.MAX_TOKENS - this_system_tokens - 4 - 2
        this_summary_tokens = estimate_used_tokens(summary_history)
        this_available_tokens = (available_tokens - 2 * this_summary_tokens) * 0.667 - 100

        partial_words = ''

        cnt = 0
        unchanged_summary = []
        while keep_summary:
            summary_size_ratio = 2 * this_summary_tokens / available_tokens
            print('summary_size_ratio:{0:.2%}'.format(summary_size_ratio))
            # æ‘˜è¦é‡éå¤§éœ€è¦ç¸®æ¸›
            if summary_size_ratio > 0.4 and not is_final_stage:
                content = summary_repository[cnt]
                this_tokens = builtins.sum([estimate_used_tokens(c) + 1 for c in content])
                part1, part2 = split_summary(content, int(this_tokens * 0.667))
                summary_history = '\n'.join(part2)
                unchanged_summary.extend(part1)
                this_summary_tokens = estimate_used_tokens(summary_history)
                this_available_tokens = (available_tokens - 2 * this_summary_tokens) * 0.667 - 100
                new_summary_size_ratio = 2 * this_summary_tokens / available_tokens
                print('summary_size_ratio  {0:.2%}=>{1:.2%}'.format(summary_size_ratio, new_summary_size_ratio))

            try:
                this_available_tokens = (
                                                available_tokens - 2 * this_summary_tokens) * 0.667 - 100 if not is_final_stage else (
                                                                                                                                         available_tokens) // 2 - 100
                # get tokens
                if len(large_inputs) == 0:
                    if is_final_stage:
                        break
                    else:
                        is_final_stage = True
                        keep_summary = False
                        available_tokens = summaryChatGpt.MAX_TOKENS - this_final_tokens - 4 - 2
                        this_summary_tokens = 0
                        this_available_tokens = (available_tokens) // 2 - 100
                        large_inputs = copy.deepcopy(unchanged_summary)
                        large_inputs.extend(summary_history.split('\n'))
                        keep_summary = True

                if not is_final_stage:
                    partial_words, large_inputs = get_next_paragraph(large_inputs, this_available_tokens)
                    remain_tokens = builtins.sum([estimate_used_tokens(w) + 1 for w in large_inputs])
                    print('partial_words:{0} large_inputs:{1}'.format(
                        builtins.sum([estimate_used_tokens(w) + 1 for w in partial_words]), remain_tokens))

                else:

                    partial_words, large_inputs = get_next_paragraph(large_inputs, this_available_tokens)
                    remain_tokens = builtins.sum([estimate_used_tokens(w) + 1 for w in large_inputs])
                    if remain_tokens < 50:
                        partial_words.extend(large_inputs)
                        remain_tokens = 0
                        large_inputs = []
                    print('partial_words:{0} large_inputs:{1}'.format(
                        builtins.sum([estimate_used_tokens(w) + 1 for w in partial_words]), remain_tokens))
                    if len(large_inputs) == 0:
                        keep_summary = False

                passage = "ç´¯ç©æ‘˜è¦æ¸…å–®:\n\n\"\"\"\n\n{0}\n\n\"\"\"\n\nè¼¸å…¥æ–‡å­—å…§å®¹:\n\n\"\"\"\n\n{1}\n\n\"\"\"\n\n".format(
                    summary_history, '\n'.join(partial_words))
                passage_final = "æ‘˜è¦æ¸…å–®:\n\n\"\"\"\n\n{0}\n\n\"\"\"\n\næ¨™è™Ÿèµ·å§‹æ•¸å­—:{1}\n".format(
                    '\n'.join(partial_words), get_last_ordered_index(cleansed_summary))

                conversation = [
                    {
                        "role": "system",
                        "content": _system_prompt if not is_final_stage else _final_prompt
                    },
                    {
                        "role": "user",
                        "content": passage if not is_final_stage else passage_final
                    }
                ]
                print(conversation)

                _max_tokens = builtins.min(summaryChatGpt.MAX_TOKENS,
                                           estimate_used_tokens(str(conversation)) + estimate_used_tokens(
                                               '\n'.join(partial_words)) * (0.3 if not is_final_stage else 1))
                _parameters['max_tokens'] = _max_tokens

                streaming_answer = summaryChatGpt.post_and_get_streaming_answer(conversation, _parameters, full_history)
                answer = ''
                answer_head = """  \n## ç¬¬{0}éƒ¨åˆ†æ‘˜è¦ {1:.2%}  \n\n\n""".format(cnt + 1, float(
                    large_inputs_tokens - remain_tokens) / large_inputs_tokens).replace('\n\n\n',
                                                                                        '\n{0} \n') if not is_final_stage else """  \n## æœ€çµ‚ç‰ˆæ‘˜è¦  \n{0} \n"""

                while True:
                    try:
                        answer, full_history = next(streaming_answer)
                        if not is_final_stage:
                            yield answer_head.format(text2markdown(('\n'.join(unchanged_summary) if len(
                                unchanged_summary) > 0 else '') + '  \n' + '  \n'.join(
                                get_rolling_summary_results(answer)))), full_history
                        else:
                            yield answer_head.format(text2markdown(('\n'.join(cleansed_summary) if len(
                                cleansed_summary) > 0 else '') + '  \n' + '  \n'.join(
                                get_rolling_summary_results(answer)))), full_history
                    except StopIteration:
                        break
                print(answer_head.format(answer))
                print('\n\n')
                if not is_final_stage:
                    summary_repository[cnt + 1] = get_rolling_summary_results(answer)
                    summary_history = '\n'.join(summary_repository[cnt + 1])
                    this_summary_tokens = estimate_used_tokens(summary_history)
                    this_available_tokens = (available_tokens - 2 * this_summary_tokens) * 0.667 - 100

                    cnt += 1
                else:
                    cleansed_summary.extend(get_rolling_summary_results(answer))
                    this_available_tokens = (available_tokens) // 2 - 100

                yield answer_head.format(text2markdown('\n'.join(
                    unchanged_summary) + '\n' + summary_history)) if not is_final_stage else answer_head.format(
                    text2markdown('\n'.join(cleansed_summary))), full_history

            except Exception as e:
                PrintException()
                raise gr.Error(str(e))

    elif summary_method == 1:
        _system_prompt = open("prompts/incremental_rolling_summary.md", encoding="utf-8").read()
        _final_prompt = open("prompts/summary_final_cleansing.md", encoding="utf-8").read()

        summary_history = 'ç©ºçš„æ¸…å–®'
        this_system_tokens = estimate_used_tokens(_system_prompt) + estimate_used_tokens('system',
                                                                                         model_name=summaryChatGpt.API_MODEL) + 4
        this_final_tokens = estimate_used_tokens(_final_prompt) + estimate_used_tokens('system',
                                                                                       model_name=summaryChatGpt.API_MODEL) + 4
        available_tokens = summaryChatGpt.MAX_TOKENS - this_system_tokens - 4 - 2
        this_summary_tokens = estimate_used_tokens(summary_history)
        this_available_tokens = (available_tokens - 2 * this_summary_tokens) * 0.667 - 100

        partial_words = ''

        cnt = 0
        unchanged_summary = []
        while keep_summary:
            summary_size_ratio = 2 * this_summary_tokens / available_tokens
            print('summary_size_ratio:{0:.2%}'.format(summary_size_ratio))
            # æ‘˜è¦é‡éå¤§éœ€è¦ç¸®æ¸›
            if summary_size_ratio > 0.4 and not is_final_stage:
                content = summary_repository[cnt]
                this_tokens = builtins.sum([estimate_used_tokens(c) + 1 for c in content])
                part1, part2 = split_summary(content, int(this_tokens * 0.667))
                summary_history = '\n'.join(part2)
                unchanged_summary.extend(part1)
                this_summary_tokens = estimate_used_tokens(summary_history)
                this_available_tokens = (available_tokens - 2 * this_summary_tokens) * 0.667 - 100
                new_summary_size_ratio = 2 * this_summary_tokens / available_tokens
                print('summary_size_ratio  {0:.2%}=>{1:.2%}'.format(summary_size_ratio, new_summary_size_ratio))

            try:
                this_available_tokens = (
                                                available_tokens - 2 * this_summary_tokens) * 0.667 - 100 if not is_final_stage else (
                                                                                                                                         available_tokens) // 2 - 100
                # get tokens
                if len(large_inputs) == 0:
                    if is_final_stage:
                        break
                    else:
                        is_final_stage = True
                        keep_summary = False
                        available_tokens = summaryChatGpt.MAX_TOKENS - this_final_tokens - 4 - 2
                        this_summary_tokens = 0
                        this_available_tokens = (available_tokens) // 2 - 100
                        large_inputs = copy.deepcopy(unchanged_summary)
                        large_inputs.extend(summary_history.split('\n'))
                        keep_summary = True

                if not is_final_stage:
                    partial_words, large_inputs = get_next_paragraph(large_inputs, this_available_tokens)
                    remain_tokens = builtins.sum([estimate_used_tokens(w) + 1 for w in large_inputs])
                    print('partial_words:{0} large_inputs:{1}'.format(
                        builtins.sum([estimate_used_tokens(w) + 1 for w in partial_words]), remain_tokens))

                else:

                    partial_words, large_inputs = get_next_paragraph(large_inputs, this_available_tokens)
                    remain_tokens = builtins.sum([estimate_used_tokens(w) + 1 for w in large_inputs])
                    if remain_tokens < 50:
                        partial_words.extend(large_inputs)
                        remain_tokens = 0
                        large_inputs = []
                    print('partial_words:{0} large_inputs:{1}'.format(
                        builtins.sum([estimate_used_tokens(w) + 1 for w in partial_words]), remain_tokens))
                    if len(large_inputs) == 0:
                        keep_summary = False

                passage = "ç´¯ç©æ‘˜è¦æ¸…å–®:\n\n\"\"\"\n\n{0}\n\n\"\"\"\n\nè¼¸å…¥æ–‡å­—å…§å®¹:\n\n\"\"\"\n\n{1}\n\n\"\"\"\n\n".format(
                    summary_history, '\n'.join(partial_words))
                passage_final = "æ‘˜è¦æ¸…å–®:\n\n\"\"\"\n\n{0}\n\n\"\"\"\n\næ¨™è™Ÿèµ·å§‹æ•¸å­—:{1}\n".format(
                    '\n'.join(partial_words), get_last_ordered_index(cleansed_summary))

                conversation = [
                    {
                        "role": "system",
                        "content": _system_prompt if not is_final_stage else _final_prompt
                    },
                    {
                        "role": "user",
                        "content": passage if not is_final_stage else passage_final
                    }
                ]
                print(conversation)

                _max_tokens = builtins.min(summaryChatGpt.MAX_TOKENS,
                                           estimate_used_tokens(str(conversation)) + estimate_used_tokens(
                                               '\n'.join(partial_words)) * (0.3 if not is_final_stage else 1))
                _parameters['max_tokens'] = _max_tokens

                streaming_answer = summaryChatGpt.post_and_get_streaming_answer(conversation, _parameters, full_history)
                answer = ''
                answer_head = """  \n## ç¬¬{0}éƒ¨åˆ†æ‘˜è¦ {1:.2%}  \n\n\n""".format(cnt + 1, float(
                    large_inputs_tokens - remain_tokens) / large_inputs_tokens).replace('\n\n\n',
                                                                                        '\n{0} \n') if not is_final_stage else """  \n## æœ€çµ‚ç‰ˆæ‘˜è¦  \n{0} \n"""

                while True:
                    try:
                        answer, full_history = next(streaming_answer)
                        if not is_final_stage:
                            yield answer_head.format(text2markdown(('\n'.join(unchanged_summary) if len(
                                unchanged_summary) > 0 else '') + '  \n' + '  \n'.join(
                                get_rolling_summary_results(answer)))), full_history
                        else:
                            yield answer_head.format(text2markdown(('\n'.join(cleansed_summary) if len(
                                cleansed_summary) > 0 else '') + '  \n' + '  \n'.join(
                                get_rolling_summary_results(answer)))), full_history
                    except StopIteration:
                        break
                    except Exception as e:
                        gr.Error(str(e))
                print(answer_head.format(answer))
                print('\n\n')
                if not is_final_stage:
                    merged_summary_history = summary_history.split('\n') if summary_history != 'ç©ºçš„æ¸…å–®' else []
                    number_list = [extract_numbered_list_member(txt) for txt in merged_summary_history]
                    max_number = 0
                    if len(merged_summary_history) > 0:
                        max_number = int(extract_numbered_list_member(merged_summary_history[-1]).split('.')[0])

                    new_summary = get_rolling_summary_results(answer)
                    for i in range(len(new_summary)):
                        this_summary = new_summary[i]
                        this_number = extract_numbered_list_member(this_summary)
                        if this_number in number_list:
                            merged_summary_history[number_list.index(this_number)] = this_summary
                        else:
                            merged_summary_history.append(this_summary)

                    summary_repository[cnt + 1] = merged_summary_history
                    summary_history = '\n'.join(summary_repository[cnt + 1])
                    this_summary_tokens = estimate_used_tokens(summary_history)
                    this_available_tokens = (available_tokens - 2 * this_summary_tokens) * 0.667 - 100

                    cnt += 1
                else:
                    cleansed_summary.extend(get_rolling_summary_results(answer))
                    this_available_tokens = (available_tokens) // 2 - 100

                yield answer_head.format(text2markdown('\n'.join(unchanged_summary) + '\n' + '  \n'.join(
                    get_rolling_summary_results(answer)))) if not is_final_stage else answer_head.format(text2markdown(
                    '\n'.join(cleansed_summary) + '\n' + '  \n'.join(
                        get_rolling_summary_results(answer)))), full_history

            except Exception as e:
                PrintException()
                raise gr.Error(str(e))
    elif summary_method == 2:
        _system_prompt = open("prompts/parallel_chunks_summary.md", encoding="utf-8").read()
        _final_prompt = open("prompts/summary_final_cleansing.md", encoding="utf-8").read()

        summary_history = 'ç©ºçš„æ¸…å–®'
        this_system_tokens = estimate_used_tokens(_system_prompt) + estimate_used_tokens('system',
                                                                                         model_name=summaryChatGpt.API_MODEL) + 4
        this_final_tokens = estimate_used_tokens(_final_prompt) + estimate_used_tokens('system',
                                                                                       model_name=summaryChatGpt.API_MODEL) + 4
        available_tokens = summaryChatGpt.MAX_TOKENS - this_system_tokens - 4 - 2
        this_summary_tokens = estimate_used_tokens(summary_history)
        this_available_tokens = (available_tokens - 2 * this_summary_tokens) * 0.667 - 100

        text_dict = OrderedDict()
        tasks = []
        cnt = 0
        while keep_summary:
            partial_words, large_inputs = get_next_paragraph(large_inputs, this_available_tokens)
            remain_tokens = builtins.sum([estimate_used_tokens(w) + 1 for w in large_inputs])
            print('partial_words:{0} large_inputs:{1}'.format(
                builtins.sum([estimate_used_tokens(w) + 1 for w in partial_words]), remain_tokens))

            summary_repository[cnt] = OrderedDict()
            summary_repository[cnt]['text'] = '\n'.join(partial_words)
            tasks.append(summarize_text('\n'.join(partial_words), _system_prompt))
            time.sleep(2)
            if len(large_inputs) == 0:
                keep_summary = False
        print('é è¨ˆåˆ‡æˆ{0}å¡Š'.format(len(tasks)))
        return_values = await asyncio.gather(*tasks)
        print(datetime.now() - st)
        print(return_values)
        for k in range(len(return_values)):
            # handle process fail
            if isinstance(return_values[k], str) and 'Error' in return_values[k]:
                _parameters = copy.deepcopy(summaryChatGpt.API_PARAMETERS)
                _parameters['temperature'] = 0.001
                _parameters['presence_penalty'] = 1.2
                passage = "è¼¸å…¥æ–‡å­—å…§å®¹:\n\n\"\"\"\n\n{0}\n\n\"\"\"\n\n".format(summary_repository[k]['text'])
                conversation = [
                    {
                        "role": "system",
                        "content": system_prompt
                    },
                    {
                        "role": "user",
                        "content": passage
                    }
                ]

                summaryChatGpt.make_response()
                payload = summaryChatGpt.parameters2payload(summaryChatGpt.API_MODEL, conversation, _parameters,
                                                            stream=False)
                response = requests.post(summaryChatGpt.BASE_URL, headers=summaryChatGpt.API_HEADERS, json=payload,
                                         stream=False)
                return_values[k] = json.loads(response.content.decode())['choices'][0]["message"]

        all_summary = aggregate_summary(return_values)
        is_final_stage = True
        keep_summary = False
        available_tokens = summaryChatGpt.MAX_TOKENS - this_final_tokens - 4 - 2
        this_summary_tokens = 0
        this_available_tokens = (available_tokens) // 2 - 100
        large_inputs = copy.deepcopy(all_summary)
        keep_summary = True
        while keep_summary:
            partial_words, large_inputs = split_summary(large_inputs, this_available_tokens)
            remain_tokens = builtins.sum([estimate_used_tokens(w) + 1 for w in large_inputs])
            print('partial_words:{0} large_inputs:{1}'.format(
                builtins.sum([estimate_used_tokens(w) + 1 for w in partial_words]), remain_tokens))
            if len(large_inputs) == 0:
                keep_summary = False
            passage_final = "æ‘˜è¦æ¸…å–®:\n\n\"\"\"\n\n{0}\n\n\"\"\"\n\næ¨™è™Ÿèµ·å§‹æ•¸å­—:{1}\n".format(
                '\n'.join(partial_words), get_last_ordered_index(cleansed_summary))

            conversation = [
                {
                    "role": "system",
                    "content": _final_prompt
                },
                {
                    "role": "user",
                    "content": passage_final
                }
            ]
            print(conversation)
            streaming_answer = summaryChatGpt.post_and_get_streaming_answer(conversation, _parameters, full_history)
            answer = ''
            answer_head = """  \n## æœ€çµ‚ç‰ˆæ‘˜è¦  \n{0} \n"""
            while True:
                try:
                    answer, full_history = next(streaming_answer)
                    yield answer_head.format(text2markdown(
                        '\n'.join(cleansed_summary) if len(cleansed_summary) > 0 else '' + '  \n' + '  \n'.join(
                            get_rolling_summary_results(answer)))), full_history
                except StopIteration:
                    break
            print(answer_head.format(answer))
            print('\n\n')

            cleansed_summary.extend(get_rolling_summary_results(answer))
            this_available_tokens = (available_tokens) // 2 - 100
            yield answer_head.format(text2markdown('\n'.join(cleansed_summary))), full_history

    if 'å¿ƒæ™ºåœ–' in summary_options:
        _system_prompt = open("prompts/mindmap_summary.md", encoding="utf-8").read()

        base_summary = copy.deepcopy(cleansed_summary)
        keep_summary = True
        this_system_tokens = estimate_used_tokens(_system_prompt) + estimate_used_tokens('system',
                                                                                         model_name=summaryChatGpt.API_MODEL) + 4
        available_tokens = summaryChatGpt.MAX_TOKENS - this_system_tokens - 4 - 2
        this_summary_tokens = estimate_used_tokens(mindmap_history)
        this_available_tokens = (available_tokens - 2 * this_summary_tokens) * 0.667 - 100
        large_inputs = base_summary
        partial_words = ''

        cnt = 0
        try:
            while keep_summary:
                this_system_tokens = estimate_used_tokens(str(mindmap_history))
                this_available_tokens = (available_tokens - this_system_tokens) - 100
                # get tokens

                partial_words, large_inputs = get_next_paragraph(large_inputs, this_available_tokens)
                print('partial_words:{0} large_inputs:{1}'.format(len(''.join(partial_words)),
                                                                  len(''.join(large_inputs))))

                passage = "æ‘˜è¦å¿ƒæ™ºåœ–:\n\n{0}\n\næ‘˜è¦æ¸…å–®:\n\n{1}\n\n".format(mindmap_history, '\n'.join(partial_words))

                conversation = [
                    {
                        "role": "system",
                        "content": _system_prompt
                    },
                    {
                        "role": "user",
                        "content": passage
                    }
                ]
                print(conversation)
                streaming_answer = summaryChatGpt.post_and_get_streaming_answer(conversation, _parameters, full_history)
                answer = ''

                while True:
                    try:
                        answer, full_history = next(streaming_answer)
                    except StopIteration:
                        break
                print(mindmap_head)
                print(answer)
                print('\n\n')
                if len(large_inputs) == 0:
                    keep_summary = False

                mindmap_history = answer
                available_tokens = int((summaryChatGpt.MAX_TOKENS - 200 - estimate_used_tokens(
                    answer) - this_system_tokens - 4 - 2) * 0.667)
                cnt += 1

        except Exception as e:
            raise gr.Error(str(e))
        yield answer_head.format(
            text2markdown('\n'.join(cleansed_summary))) + '\n\n\n' + mindmap_head + '\n' + mindmap_history, full_history

    if 'æœƒè­°è¨˜éŒ„' in summary_options:
        _system_prompt = open("prompts/meeting_minutes_summary.md", encoding="utf-8").read()
        base_summary = copy.deepcopy(cleansed_summary)
        keep_summary = True
        this_system_tokens = estimate_used_tokens(_system_prompt) + estimate_used_tokens('system',
                                                                                         model_name=summaryChatGpt.API_MODEL) + 4
        this_final_tokens = estimate_used_tokens(_final_prompt) + estimate_used_tokens('system',
                                                                                       model_name=summaryChatGpt.API_MODEL) + 4
        available_tokens = summaryChatGpt.MAX_TOKENS - this_system_tokens - 4 - 2
        this_summary_tokens = estimate_used_tokens(meeting_minutes)
        this_available_tokens = (available_tokens - 2 * this_summary_tokens) * 0.667 - 100
        large_inputs = base_summary
        partial_words = ''
        unchanged_summary = []
        cnt = 0
        try:
            while keep_summary:
                this_system_tokens = estimate_used_tokens(str(meeting_minutes))
                this_available_tokens = (available_tokens - this_system_tokens) - 100
                # get tokens

                partial_words, large_inputs = get_next_paragraph(large_inputs, this_available_tokens)
                print('partial_words:{0} large_inputs:{1}'.format(len(''.join(partial_words)),
                                                                  len(''.join(large_inputs))))

                passage = "æœƒè­°è¨˜éŒ„é‡é»:\n\n{0}\n\næ‘˜è¦æ¸…å–®:\n\n{1}\n\n".format(meeting_minutes,
                                                                                '\n'.join(partial_words))

                conversation = [
                    {
                        "role": "system",
                        "content": _system_prompt
                    },
                    {
                        "role": "user",
                        "content": passage
                    }
                ]
                print(conversation)
                streaming_answer = summaryChatGpt.post_and_get_streaming_answer(conversation, _parameters, full_history)
                answer = ''
                meeting_head = '# æœƒè­°è¨˜éŒ„'
                while True:
                    try:
                        answer, full_history = next(streaming_answer)
                    except StopIteration:
                        break
                print(meeting_head)
                print(answer)
                print('\n\n')
                if len(large_inputs) == 0:
                    keep_summary = False

                meeting_minutes = answer
                available_tokens = int((summaryChatGpt.MAX_TOKENS - 200 - estimate_used_tokens(
                    answer) - this_system_tokens - 4 - 2) * 0.667)
                cnt += 1

        except Exception as e:
            raise gr.Error(str(e))
        yield answer_head.format(text2markdown('\n'.join(
            cleansed_summary))) + '\n\n\n' + mindmap_head + '\n' + mindmap_history + '\n\n\n' + meeting_head + '\n' + meeting_minutes, full_history

    if 'é‡é»ä¸»é¡Œ' in summary_options:
        _system_prompt = open("prompts/topic_driven_summary.md", encoding="utf-8").read()
        base_summary = copy.deepcopy(cleansed_summary)
        keep_summary = True
        this_system_tokens = estimate_used_tokens(_system_prompt) + estimate_used_tokens('system',
                                                                                         model_name=summaryChatGpt.API_MODEL) + 4
        this_final_tokens = estimate_used_tokens(_final_prompt) + estimate_used_tokens('system',
                                                                                       model_name=summaryChatGpt.API_MODEL) + 4
        available_tokens = summaryChatGpt.MAX_TOKENS - this_system_tokens - 4 - 2
        this_summary_tokens = estimate_used_tokens(topic_shortcuts)
        this_available_tokens = (available_tokens - 2 * this_summary_tokens) * 0.667 - 100
        large_inputs = base_summary
        partial_words = ''

        cnt = 0
        try:
            while keep_summary:
                this_system_tokens = estimate_used_tokens(str(topic_shortcuts))
                this_available_tokens = (available_tokens - this_system_tokens) - 100
                # get tokens

                partial_words, large_inputs = get_next_paragraph(large_inputs, this_available_tokens)
                print('partial_words:{0} large_inputs:{1}'.format(len(''.join(partial_words)),
                                                                  len(''.join(large_inputs))))

                passage = "é‡é»ä¸»é¡Œ:\n\n{0}\n\næ‘˜è¦æ¸…å–®:\n\n{1}\n\n".format(topic_shortcuts, '\n'.join(partial_words))

                conversation = [
                    {
                        "role": "system",
                        "content": _system_prompt
                    },
                    {
                        "role": "user",
                        "content": passage
                    }
                ]
                print(conversation)
                streaming_answer = summaryChatGpt.post_and_get_streaming_answer(conversation, _parameters, full_history)
                answer = ''

                while True:
                    try:
                        answer, full_history = next(streaming_answer)
                    except StopIteration:
                        break
                print(topic_head)
                print(answer)
                print('\n\n')
                if len(large_inputs) == 0:
                    keep_summary = False
                topic_shortcuts = answer
                available_tokens = int((summaryChatGpt.MAX_TOKENS - 200 - estimate_used_tokens(
                    topic_shortcuts) - this_system_tokens - 4 - 2) * 0.667)
                cnt += 1

        except Exception as e:
            raise gr.Error(str(e))
        yield answer_head.format(text2markdown('\n'.join(cleansed_summary))) + '\n\n\n' + mindmap_history, full_history


def estimate_tokens(text, text2, state):
    text = '' if text is None else text
    text2 = '' if text2 is None else text2
    t1 = 'è¼¸å…¥æ–‡æœ¬é•·åº¦ç‚º{0},é è¨ˆè€—ç”¨tokensæ•¸ç‚º:{1}'.format(len(text),
                                                           estimate_used_tokens(text, summaryChatGpt.API_MODEL) + 4)
    if len(text2) == 0:
        return t1, state
    else:
        t2 = 'è¼¸å‡ºæ–‡æœ¬é•·åº¦ç‚º{0},é è¨ˆè€—ç”¨tokensæ•¸ç‚º:{1}'.format(len(text2), estimate_used_tokens(text2,
                                                                                                summaryChatGpt.API_MODEL) + 4)
        return t1 + '\t\t' + t2, state


def reformat_freq(sr, y):
    if sr not in (
            48000,
            16000,
    ):  # Deepspeech only supports 16k, (we convert 48k -> 16k)
        raise ValueError("Unsupported rate", sr)
    if sr == 48000:
        y = (
            ((y / max(np.max(y), 1)) * 32767)
            .reshape((-1, 3))
            .mean(axis=1)
            .astype("int16")
        )
        sr = 16000
    return sr, y


def transcribe(audio, need_timestamp=False, state=None):
    # if audio == None : return ""
    time.sleep(2)
    print(datetime.now(), audio)

    # _, y = reformat_freq(*audio)
    # phrase_complete=True
    # if state is None:
    #     state=[]
    # if len(state)==0:
    #     state.append(OrderedDict())
    #     state[0]['phrase_time']=  None
    #     state[0]['last_sample'] = bytes()
    #
    #     state[0]['data_queue'] = Queue()
    #     state[0]['phrase_complete']=False
    # now = datetime.utcnow()
    # Pull raw recorded audio from the queue.
    # if not state[0]['data_queue'].empty():
    #     state[0]['phrase_complete'] = False
    #     # If enough time has passed between recordings, consider the phrase complete.
    #     # Clear the current working audio buffer to start over with the new data.
    #     if state[0]['phrase_time'] and now - state[0]['phrase_time'] > timedelta(seconds=phrase_timeout):
    #         state[0]['last_sample']  = bytes()
    #         state[0]['phrase_complete'] = True
    #     # This is the last time we received new audio data from the queue.
    #     state[0]['phrase_time']  = now

    # Concatenate our current audio data with the latest audio data.
    # while not state[0]['data_queue'].empty():
    #     data = state[0]['data_queue'].get()
    #     state[0]['last_sample'] += data

    # while True:
    try:
        results = recognize_whisper(audio_data=audio, word_timestamps=need_timestamp)
        state.append(results)
        if len(state[-1]['text'] if len(state) > 0 else '') > 0:
            print(state[-1]['text'] if len(state) > 0 else '')

        return '\n'.join([result['text'] for result in state if len(result['text']) > 0]) if len(
            state) > 0 else '', state

    except KeyboardInterrupt:
        return '\n'.join([result['text'] for result in state if len(result['text']) > 0]) if len(
            state) > 0 else '', state


def update_rolling_state(state):
    return '\n'.join([result['text'] for result in state if len(result['text']) > 0]) if len(state) > 0 else '', state


def SpeechToText(audio, need_timestamp=False, state=None):
    if audio == None: return ""
    time.sleep(1)

    audio = whisper.load_audio(audio)
    audio = whisper.pad_or_trim(audio)

    # make log-Mel spectrogram and move to the same device as the model
    mel = whisper.log_mel_spectrogram(audio).to(model.device)

    # Detect the Max probability of language ?
    _, probs = model.detect_language(mel)
    language = max(probs, key=probs.get)

    #  Decode audio to Text
    options = whisper.DecodingOptions(fp16=False)
    result = whisper.decode(model, mel, options)
    return (language, result.text)




def process_audio_file(file, state, initial_prompt, need_timestamp=False):
    if file is None:
        return '', state
    else:
        folder, filename, ext = context.split_path(file.name)
        transcript = ""
        chunk_start = 0
        if ext.lower() in ['.mp4', '.avi']:
            import moviepy.editor
            video = moviepy.editor.VideoFileClip(file.name)
            audio = video.audio
            context.make_dir_if_need(os.path.join(cxt.get_prompt4all_dir(), 'audio', filename + '.wav'))
            audio.write_audiofile(os.path.join(cxt.get_prompt4all_dir(), 'audio', filename + '.wav'))
            audio_file = AudioSegment.from_wav(os.path.join(cxt.get_prompt4all_dir(), 'audio', filename + '.wav'))
        elif ext.lower() in ['.mp3']:
            audio_file = AudioSegment.from_mp3(file.name)
        elif ext.lower() in ['.wav']:
            audio_file = AudioSegment.from_wav(file.name)
        load_whisper_model()
        # audio_samples = np.array(audio_file.get_array_of_samples() )   # ç²å–æ¡æ¨£é»æ•¸æ“šé™£åˆ—
        # audio_samples = audio_samples.reshape( (-1, audio_file.channels))
        # rms = np.sqrt(np.mean(audio_samples ** 2, axis=-1))
        #
        # ref = 2 ** (8 * audio_file.sample_width - 1)  # è¨ˆç®—åƒè€ƒå€¼
        # dBFS = 20 * np.log10(np.abs(samples) / ref)  # è¨ˆç®—æ¯å€‹æ¡æ¨£é»çš„åˆ†è²æ•¸

        chunk_size = 100 * 1000  # 100 ç§’
        chunks = [audio_file[i:i + chunk_size] for i in range(0, len(audio_file), chunk_size)]
        for chunk in chunks:
            dbfs = chunk.dBFS
            if dbfs == -math.inf or dbfs < -30:
                chunk_start += chunk.duration_seconds
                pass
            else:
                with chunk.export("temp.wav", format="wav") as f:
                    result = cxt.whisper_model.transcribe("temp.wav", word_timestamps=need_timestamp, verbose=False,
                                                          language="zh", fp16=False,
                                                          no_speech_threshold=0.5, logprob_threshold=-1,
                                                          temperature=0.2,
                                                          initial_prompt="#zh-tw æœƒè­°é€å­—ç¨¿ã€‚" + initial_prompt)

                    for seg in result["segments"]:
                        if need_timestamp:
                            start, end, text = seg["start"] + chunk_start, seg["end"] + chunk_start, seg["text"]
                            if len(text) == 0:
                                pass
                            else:
                                line = f"[{to_formated_time(start)} --> {to_formated_time(end)} {text}"
                                print(line, flush=True)
                                transcript += line + '\n'
                        else:
                            if len(seg['text']) == 0:
                                pass
                            else:
                                print('{0}'.format(seg['text']), flush=True)
                                transcript += '{0}'.format(seg['text']) + '\n'

                    chunk_start += chunk.duration_seconds
            yield transcript, state
        yield transcript, state


def clear_history():
    FULL_HISTORY = [{"role": "system", "content": baseChatGpt.SYSTEM_MESSAGE,
                     "estimate_tokens": estimate_used_tokens(baseChatGpt.SYSTEM_MESSAGE,
                                                             model_name=baseChatGpt.API_MODEL)}]
    return [], FULL_HISTORY, FULL_HISTORY


def reset_textbox():
    return gr.Textbox(placeholder="ä»€éº¼æ˜¯LLM?",value="",
                                                        label="è¼¸å…¥æ–‡å­—å¾ŒæŒ‰enter", lines=10, max_lines=2000)


def reset_context():
    return gr.Dropdown(
                                        ["[@PROMPT] ä¸€èˆ¬æŒ‡ä»¤", "[@GLOBAL] å…¨å±€æŒ‡ä»¤", "[@SKIP] è·³è„«ä¸Šæ–‡",
                                         "[@SANDBOX] æ²™ç®±éš”çµ•",
                                         "[@EXPLAIN] è§£é‡‹ä¸Šæ–‡", "[@OVERRIDE] è¦†å¯«å…¨å±€"],
                                        value="[@PROMPT] ä¸€èˆ¬æŒ‡ä»¤", type='index', label="contextè™•ç†",
                                        elem_id='context_type',
                                        interactive=True)


def pause_message():
    is_pause = True


if __name__ == '__main__':
    PORT = find_available_port(7860)
    title = """<h1 align="center">ğŸ”¥ğŸ¤–Prompt is All You Need! ğŸš€</h1>"""
    if "OPENAI_API_KEY" not in os.environ:
        title = """<h1 align="center">ğŸ”¥ğŸ¤–Prompt is All You Need! ğŸš€</h1><br><h2 align="center"><span style='color:red'>ä½ å°šæœªè¨­ç½®api key</span></h2>"""
    description = ""
    cancel_handles = []
    with gr.Blocks(title="Prompt is what you need!", css=advanced_css, analytics_enabled=False,
                   theme=adjust_theme()) as demo:
        baseChatGpt = GptBaseApi(model="gpt-4-1106-preview")
        summaryChatGpt = GptBaseApi(model="gpt-3.5-turbo-16k-0613")
        imageChatGpt = GptBaseApi(model="gpt-4-1106-preview")
        otherChatGpt = GptBaseApi(model="gpt-4-1106-preview")
        state = gr.State([{"role": "system", "content": 'æ‰€æœ‰å…§å®¹ä»¥ç¹é«”ä¸­æ–‡æ›¸å¯«',
                           "estimate_tokens": estimate_used_tokens('æ‰€æœ‰å…§å®¹ä»¥ç¹é«”ä¸­æ–‡æ›¸å¯«',
                                                                   model_name=baseChatGpt.API_MODEL)}])  # s
        cxt.baseChatGpt=baseChatGpt
        cxt.summaryChatGpt=summaryChatGpt
        cxt.imageChatGpt=imageChatGpt
        cxt.otherChatGpt=otherChatGpt
        cxt.state = state
        baseChatGpt.FULL_HISTORY = state.value
        gr.HTML(title)

        with gr.Tabs():
            with gr.TabItem("å°è©±"):
                with gr.Row():
                    with gr.Tabs():
                        with gr.TabItem("èŠå¤©"):
                            with gr.Column(scale=1):
                                with gr.Row():
                                    inputs = gr.Textbox(placeholder="ä»€éº¼æ˜¯LLM?",
                                                        label="è¼¸å…¥æ–‡å­—å¾ŒæŒ‰enter", lines=10, max_lines=2000)  # t
                                    context_type = gr.Dropdown(
                                        ["[@PROMPT] ä¸€èˆ¬æŒ‡ä»¤", "[@GLOBAL] å…¨å±€æŒ‡ä»¤", "[@SKIP] è·³è„«ä¸Šæ–‡",
                                         "[@SANDBOX] æ²™ç®±éš”çµ•",
                                         "[@EXPLAIN] è§£é‡‹ä¸Šæ–‡", "[@OVERRIDE] è¦†å¯«å…¨å±€"],
                                        value="[@PROMPT] ä¸€èˆ¬æŒ‡ä»¤", type='index', label="contextè™•ç†",
                                        elem_id='context_type',
                                        interactive=True)

                                with gr.Row(variant="panel"):
                                    b1 = gr.Button(value='é€å‡º', interactive=True, size='sm', scale=1)
                                    b2 = gr.Button(value='ä¸­æ­¢', interactive=True, size='sm', scale=1)

                                with gr.Row(variant="panel"):
                                    b4 = gr.Button(value='new chat', interactive=True, size='sm', scale=1)
                                    b3 = gr.ClearButton([inputs], interactive=True, size='sm', scale=1)

                                with gr.Accordion("è¶…åƒæ•¸", open=False):
                                    top_p = gr.Slider(minimum=-0, maximum=1.0, value=1, step=0.05, interactive=True,
                                                      label="é™åˆ¶å–æ¨£ç¯„åœ(Top-p)", )
                                    temperature = gr.Slider(minimum=-0, maximum=2.0, value=0.9, step=0.1,
                                                            interactive=True,
                                                            label="æº«åº¦ (Temperature)", )
                                    top_k = gr.Slider(minimum=1, maximum=50, value=1, step=1, interactive=True,
                                                      label="å€™é¸çµæœå€‹æ•¸(Top-k)", )
                                    frequency_penalty = gr.Slider(minimum=-2, maximum=2, value=0, step=0.01,
                                                                  interactive=True,
                                                                  label="é‡è¤‡æ€§è™•ç½°(Frequency Penalty)",
                                                                  info='å€¼åŸŸç‚º-2~+2ï¼Œæ•¸å€¼è¶Šå¤§ï¼Œå°æ–¼é‡è¤‡ç”¨å­—æœƒçµ¦äºˆæ‡²ç½°ï¼Œæ•¸å€¼è¶Šè² ï¼Œå‰‡é¼“å‹µé‡è¤‡ç”¨å­—')
                        with gr.TabItem("å°è©±ç´€éŒ„"):
                            with gr.Column(elem_id="col_container"):
                                conversation_history_share_btm = gr.Button('æ›´å', scale=1, size='sm')
                                conversation_history_delete_btm = gr.Button('åˆªé™¤', scale=1, size='sm')

                            with gr.Row():
                                history_list = gr.templates.List(value=cxt.conversation_history.titles, height=550,
                                                                 headers=["æ­·å²"], datatype=["str"],
                                                                 col_count=(1, 'fixed'), interactive=True)
                    with gr.Column(scale=4, elem_id="col_container"):
                        chatbot = gr.Chatbot(elem_id='chatbot', container=True, scale=1, height=550,render_markdown=True,
                                             show_copy_button=True,bubble_full_width=True,show_share_button=True,layout="panel")
                        b3.add(chatbot)

            with gr.TabItem("æ­·å²"):
                with gr.Column(elem_id="col_container"):
                    history_viewer = gr.JSON(elem_id='history_viewer')
            with gr.TabItem("NLU"):
                with gr.Column(elem_id="col_container"):
                    gr.Markdown(
                        "å°‡æ–‡æœ¬è¼¸å…¥åˆ°ä¸‹é¢çš„æ–¹å¡Šä¸­ï¼ŒæŒ‰ä¸‹ã€Œé€å‡ºã€æŒ‰éˆ•å°‡æ–‡æœ¬é€£åŒä¸Šè¿°çš„promptç™¼é€è‡³OpenAI ChatGPT APIï¼Œç„¶å¾Œå°‡è¿”å›çš„JSONé¡¯ç¤ºåœ¨è¦–è¦ºåŒ–ç•Œé¢ä¸Šã€‚")
                    with gr.Row():
                        with gr.Column(scale=1):
                            nlu_inputs = gr.Textbox(lines=6, placeholder="è¼¸å…¥å¥å­...")
                        with gr.Column(scale=2):
                            nlu_output = gr.Text(label="å›å‚³çš„JSONè¦–è¦ºåŒ–", interactive=True, max_lines=40,
                                                 show_copy_button=True)
                    nlu_button = gr.Button("é€å‡º")
            with gr.TabItem("Dall.E3"):
                with gr.Column(variant="panel"):
                    with gr.Row(variant="compact"):
                        image_text = gr.Textbox(
                            label="è«‹è¼¸å…¥ä¸­æ–‡çš„æè¿°",
                            show_label=False,
                            max_lines=1,
                            placeholder="è«‹è¼¸å…¥ä¸­æ–‡çš„æè¿°",
                            container=False
                        )
                    image_btn = gr.Button("è¨­è¨ˆèˆ‡ç”Ÿæˆåœ–ç‰‡", scale=1)
                    image_prompt = gr.Markdown("")
                    image_gallery = gr.Gallery(value=None, show_label=False, columns=[4], object_fit="contain",
                                               height="auto")
                with gr.Accordion("è¶…åƒæ•¸", open=False):
                    temperature2 = gr.Slider(minimum=-0, maximum=2.0, value=0.7, step=0.1, interactive=True,
                                             label="æº«åº¦ (Temperature)", )
                    image_size = gr.Radio([1024], label="åœ–ç‰‡å°ºå¯¸", value=1024)
            with gr.TabItem("é¢¨æ ¼æ”¹å¯«"):
                rewrite_ui.rewrite_panel()
            with gr.TabItem("é•·æ–‡æœ¬æ‘˜è¦"):
                with gr.Tabs():
                    with gr.TabItem("é•·æ–‡æœ¬è™•ç†"):
                        rolling_state = gr.State([])
                        text_statistics = gr.Markdown()
                        with gr.Row():
                            with gr.Column(scale=1):
                                with gr.Row():
                                    with gr.Tabs():
                                        with gr.TabItem("æ–‡å­—"):
                                            rolliing_source_file = gr.File(value=None, file_count="single",
                                                                           label='è«‹å°‡æª”æ¡ˆæ‹–æ›³è‡³æ­¤æˆ–æ˜¯é»æ“Šå¾Œä¸Šå‚³',
                                                                           file_types=[".txt", ".json", ".csv", ".pdf"],
                                                                           scale=2,
                                                                           elem_id='rolling_file')
                                        with gr.TabItem("å½±éŸ³"):
                                            whisper_timestamp_checkbox1 = gr.Checkbox(label="é™„åŠ æ™‚é–“æˆ³", value=True,
                                                                                      scale=1)
                                            initial_prompt_textbox = gr.Textbox(
                                                placeholder="è«‹è¼¸å…¥æè¿°å½±éŸ³å…§å®¹çš„åˆå§‹prompt", label="åˆå§‹prompt")
                                            audio_source_file = gr.File(value=None, file_count="single",
                                                                        label='è«‹å°‡æª”æ¡ˆæ‹–æ›³è‡³æ­¤æˆ–æ˜¯é»æ“Šå¾Œä¸Šå‚³',
                                                                        file_types=[".mp3", ".mp4", ".avi", ".wav"],
                                                                        scale=2,
                                                                        elem_id='rolling_file')

                                        with gr.TabItem("å³æ™‚whisper"):
                                            with gr.Row():
                                                whisper_state = gr.State([])
                                                whisper_timestamp_checkbox = gr.Checkbox(label="é™„åŠ æ™‚é–“æˆ³",
                                                                                         value=False, scale=1)
                                                rolling_audio = gr.Button('ğŸ™ï¸', size='sm', )
                                                invisible_whisper_text = gr.Text(visible=False)
                                        with gr.TabItem("Arxiv"):
                                            gr.Textbox(label="è«‹è¼¸å…¥Arxivå®Œæ•´ç¶²å€æˆ–æ˜¯è«–æ–‡ç·¨è™Ÿ")
                                        with gr.TabItem("Youtube"):
                                            gr.Textbox(label="è«‹è¼¸å…¥Youtubeå½±ç‰‡å®Œæ•´ç¶²å€")
                                            gr.Radio(["å­—å¹•æª”", "éŸ³æª”è½‰æ–‡å­—"], label="ä¿¡æ¯ä¾†æº")
                            with gr.Column(scale=1):
                                with gr.Group():
                                    summary_radio = gr.Dropdown(
                                        ["æ»¾å‹•å¼æ•´åˆæ‘˜è¦", "æ»¾å‹•å¼ç´¯åŠ æ‘˜è¦", "å¹³è¡Œåˆ†å¡Šæ‘˜è¦"], multiselect=False,
                                        label="æ‘˜è¦æŠ€è¡“", type="index",
                                        value="æ»¾å‹•å¼æ•´åˆæ‘˜è¦", interactive=True, min_width=150)
                                    summary_options = gr.CheckboxGroup(["å¿ƒæ™ºåœ–", "æœƒè­°è¨˜éŒ„", "é‡é»ä¸»é¡Œ"],
                                                                       label="è¼”åŠ©åŠŸèƒ½")
                                    rolling_button = gr.Button("â–¶ï¸", size='sm', scale=1, min_width=80)
                                    rolling_clear_button = gr.ClearButton([rolliing_source_file], value="ğŸ—‘ï¸", size='sm',
                                                                          scale=1, min_width=80)
                                    rolling_cancel_button = gr.Button("â¹ï¸", size='sm', scale=1, min_width=80)

                        with gr.Row():
                            with gr.Column(scale=1):
                                large_inputs = gr.Text(label="ä¾†æºæ–‡å­—", lines=30, max_lines=5000)
                            with gr.Column(scale=1, elem_id="col_container"):
                                summary_output = gr.Markdown(label="æ‘˜è¦", elem_classes='markdown')
                            rolling_clear_button.add(large_inputs)
                            rolling_clear_button.add(summary_output)
                    with gr.TabItem("å­˜æª”"):
                        with gr.Column(elem_id="col_container"):
                            with gr.Row():
                                file_obj = gr.File(label="æ‘˜è¦æª”", file_types=[".md"], value=None, interactive=False,
                                                   min_width=60, show_label=False)
                                rolling_save_button = gr.Button("ğŸ’¾", size='sm', scale=1)
                    with gr.TabItem("ç´€éŒ„"):
                        with gr.Column(elem_id="col_container"):
                            rolling_history_viewer = gr.JSON(elem_id='rolling_history_viewer')
            with gr.TabItem("è¨­å®š"):
                with gr.Column():

                    dropdown_api1 = gr.Dropdown(choices=[k for k in model_info.keys()], value="gpt-4-1106-preview",
                                                label="å°è©±ä½¿ç”¨ä¹‹api", interactive=True)
                    dropdown_api4 = gr.Dropdown(choices=[k for k in model_info.keys()], value="gpt-4-1106-preview",
                                                label="ä»¥æ–‡ç”Ÿåœ–ä½¿ç”¨ä¹‹api", interactive=True)
                    dropdown_api2 = gr.Dropdown(choices=[k for k in model_info.keys()], value="gpt-3.5-turbo-16k-0613",
                                                label="é•·æ–‡æœ¬æ‘˜è¦ä½¿ç”¨ä¹‹api", interactive=True)
                    dropdown_api3 = gr.Dropdown(choices=[k for k in model_info.keys()], value="gpt-4-1106-preview",
                                                label="å…¶ä»–åŠŸèƒ½ä½¿ç”¨ä¹‹api", interactive=True)
                    gr.Group(dropdown_api1,dropdown_api4,dropdown_api2,dropdown_api3)
                    settings_panel.database_query_panel()







        inputs_event = inputs.submit(prompt_api,
                                     [inputs, context_type, top_p, temperature, top_k, frequency_penalty, state],
                                     [chatbot, state, history_viewer])
        cancel_handles.append(inputs_event)
        inputs_event.then(reset_context, [], [context_type]).then(reset_textbox, [], [inputs])
        b1_event = b1.click(prompt_api, [inputs, context_type, top_p, temperature, top_k, frequency_penalty, state],
                            [chatbot, state, history_viewer])
        cancel_handles.append(b1_event)
        b1_event.then(reset_context, [], [context_type]).then(reset_textbox, [], [inputs])
        b3.click(clear_history, [], [chatbot, state, history_viewer]).then(reset_textbox, [], [inputs])
        b2.click(fn=None, inputs=None, outputs=None, cancels=cancel_handles)
        chatbot.change(scroll_to_output=True)


        def new_chat(state):
            cxt.conversation_history.new_chat()
            history_list.value = cxt.conversation_history.titles
            chat=cxt.conversation_history.selected_item.get_gradio_chat()
            return chat,state


        b4.click(fn=new_chat, inputs=[state], outputs=[chatbot, state])

        nlu_inputs.submit(nlu_api, nlu_inputs, nlu_output)
        nlu_button.click(nlu_api, nlu_inputs, nlu_output)

        image_text.submit(image_api, [image_text, image_size, temperature2], [image_prompt, image_gallery])
        image_btn.click(image_api, [image_text, image_size, temperature2], [image_prompt, image_gallery])



        rolling_cancel_handel = []

        rolling_inputs_event = rolling_button.click(rolling_summary,
                                                    [large_inputs, rolling_state, summary_radio, summary_options],
                                                    [summary_output, rolling_state]).then(estimate_tokens,
                                                                                          [large_inputs, summary_output,
                                                                                           rolling_state],
                                                                                          [text_statistics,
                                                                                           rolling_state])
        # large_inputs.submit(rolling_summary.md, [large_inputs, rolling_state,rolling_parallel_checkbox], [summary_output, rolling_state]).then(estimate_tokens, [large_inputs,summary_output, rolling_state],[text_statistics,rolling_state])
        large_inputs_change_event = large_inputs.change(estimate_tokens, [large_inputs, summary_output, rolling_state],
                                                        [text_statistics, rolling_state])
        source_file_change_event = rolliing_source_file.change(process_file, [rolliing_source_file, rolling_state],
                                                               [large_inputs, rolling_state])
        audio_file_change_event = audio_source_file.change(process_audio_file,
                                                           [audio_source_file, whisper_state, initial_prompt_textbox,
                                                            whisper_timestamp_checkbox1], [large_inputs, whisper_state])
        rolling_cancel_handel.append(rolling_inputs_event)
        rolling_cancel_handel.append(large_inputs_change_event)
        rolling_cancel_handel.append(source_file_change_event)
        rolling_cancel_handel.append(audio_file_change_event)
        rolling_cancel_button.click(fn=None, inputs=None, outputs=None, cancels=rolling_cancel_handel)


        def select_conversation(evt: gr.SelectData):
            conversations = cxt.conversation_history.conversations[evt.index[0]].get_prompt_messages(only_final=True)
            cxt.conversation_history.selected_index = evt.index[0]
            chat=cxt.conversation_history.selected_item.get_gradio_chat()
            return chat,cxt.state


        history_list.select(select_conversation, None, [chatbot, state])


        def save_file(contents, state):
            text_file = "generate_text/summary_{0}.txt".format(
                str(datetime.now()).replace(' ', '').replace(':', '').replace('-', '').replace('.', ''))
            if summary_radio.value == 0:
                text_file = "generate_text/rolling_summary_{0}.txt".format(
                    str(datetime.now()).replace(' ', '').replace(':', '').replace('-', '').replace('.', ''))
            elif summary_radio.value == 1:
                text_file = "generate_text/incremental_rolling_summary_{0}.txt".format(
                    str(datetime.now()).replace(' ', '').replace(':', '').replace('-', '').replace('.', ''))
            elif summary_radio.value == 2:
                text_file = "generate_text/parallel_summary_{0}.txt".format(
                    str(datetime.now()).replace(' ', '').replace(':', '').replace('-', '').replace('.', ''))
            elif summary_radio.value == 3:
                text_file = "generate_text/mindmap_summary_{0}.txt".format(
                    str(datetime.now()).replace(' ', '').replace(':', '').replace('-', '').replace('.', ''))
            elif summary_radio.value == 4:
                text_file = "generate_text/meeting_summary_{0}.txt".format(
                    str(datetime.now()).replace(' ', '').replace(':', '').replace('-', '').replace('.', ''))
            elif summary_radio.value == 5:
                text_file = "generate_text/topic_summary_{0}.txt".format(
                    str(datetime.now()).replace(' ', '').replace(':', '').replace('-', '').replace('.', ''))

            with open(text_file, 'w', encoding='utf-8') as f:
                f.write(contents)
            return text_file, state


        rolling_save_button.click(save_file, [summary_output, rolling_state], [file_obj, rolling_state])

        invisible_whisper_text.change(update_rolling_state, [whisper_state], [large_inputs, rolling_history_viewer])

        dropdown_api1.change(lambda x: baseChatGpt.change_model(x), [dropdown_api1], [])
        dropdown_api2.change(lambda x: summaryChatGpt.change_model(x), [dropdown_api2], [])
        dropdown_api3.change(lambda x: otherChatGpt.change_model(x), [dropdown_api3], [])
        dropdown_api4.change(lambda x: imageChatGpt.change_model(x), [dropdown_api4], [])

        gr.Markdown(description)


        # gradioçš„inbrowserè§¦å‘ä¸å¤ªç¨³å®šï¼Œå›æ»šä»£ç åˆ°åŸå§‹çš„æµè§ˆå™¨æ‰“å¼€å‡½æ•°
        def auto_opentab_delay():
            import threading, webbrowser, time
            print(f"è‹¥æ˜¯ç€è¦½å™¨æœªè‡ªå‹•é–‹å•Ÿï¼Œè«‹ç›´æ¥é»é¸ä»¥ä¸‹é€£çµï¼š")
            print(f"\tï¼ˆæš—é»‘æ¨¡å¼ï¼‰: http://localhost:{PORT}/?__theme=dark")
            print(f"\tï¼ˆå…‰æ˜æ¨¡å¼ï¼‰: http://localhost:{PORT}")

            def open():
                time.sleep(2)  # æ‰“å¼€æµè§ˆå™¨
                DARK_MODE = True
                if DARK_MODE:
                    webbrowser.open_new_tab(f"http://localhost:{PORT}/?__theme=dark")
                else:
                    webbrowser.open_new_tab(f"http://localhost:{PORT}")

            threading.Thread(target=open, name="open-browser", daemon=True).start()


        auto_opentab_delay()
        demo.queue( api_open=False).launch(show_error=True, max_threads=200, share=True,server_port=PORT)
