# -*- coding: utf-8-sig -*-
import json
import os
import regex
import gradio as gr
import openai
import copy
import requests
import asyncio
import time
import nest_asyncio
import openai_async
nest_asyncio.apply()
from collections import OrderedDict
from datetime import datetime
from prompt4all.utils.chatgpt_utils import *
from prompt4all.utils.regex_utils import *
import prompt4all.api.context_type as ContextType
from prompt4all.api.base_api import *
from prompt4all.utils.tokens_utils import *
#from gradio_chatbot_patch import Chatbot as grChatbot
# from gradio_css import code_highlight_css
from prompt4all.theme import adjust_theme, advanced_css

os.environ['no_proxy'] = '*'

# è¨­ç½®æ‚¨çš„OpenAI APIé‡‘é‘°
# è«‹å°‡æ‚¨çš„é‡‘é‘°å€¼å¯«å…¥è‡³ç’°å¢ƒè®Šæ•¸"OPENAI_API_KEY"ä¸­
# os.environ['OPENAI_API_KEY']=#'ä½ çš„é‡‘é‘°å€¼'
if "OPENAI_API_KEY" not in os.environ:
    print("OPENAI_API_KEY  is not exists!")
openai.api_key = os.getenv("OPENAI_API_KEY")
URL = "https://api.openai.com/v1/chat/completions"

pattern = regex.compile(r'\{(?:[^{}]|(?R))*\}')


def index2context(idx: int):
    if idx is None or idx == 0:
        return ContextType.prompt
    elif idx == 1:
        return ContextType.globals
    elif idx == 2:
        return ContextType.skip
    elif idx == 3:
        return ContextType.sandbox
    elif idx == 4:
        return ContextType.explain
    elif idx == 5:
        return ContextType.override
    else:
        return '[@PROMPT]'


def prompt_api(inputs, context_type, top_p, temperature, top_k, frequency_penalty, full_history=[]):
    _context_type = index2context(context_type)
    baseChatGpt.API_PARAMETERS['temperature'] = temperature
    baseChatGpt.API_PARAMETERS['top_p'] = top_p
    baseChatGpt.API_PARAMETERS['top_k'] = top_k
    baseChatGpt.API_PARAMETERS['frequency_penalty'] = frequency_penalty
    streaming_chat = baseChatGpt.post_a_streaming_chat(inputs, _context_type, baseChatGpt.API_PARAMETERS, full_history)
    while True:
        try:
            chat, answer, full_history = next(streaming_chat)
            yield chat, full_history, full_history
        except StopIteration:
            break
        except Exception as e:
            raise ValueError(e)


def nlu_api(text_input):
    # å‰µå»ºèˆ‡APIçš„å°è©±

    text_inputs = text_input.split('\n')
    _parameters = copy.deepcopy(baseChatGpt.API_PARAMETERS)
    _parameters['temperature'] = 0.1
    results = []
    for txt in text_inputs:
        conversation = [
            {
                "role": "system",
                "content": "è«‹é€ä¸€è®€å–ä¸‹åˆ—å¥å­ï¼Œæ¯å€‹å¥å­å…ˆç†è§£èªæ„å¾Œå†é€²è¡Œåˆ†è©ã€æƒ…æ„Ÿåµæ¸¬ã€å‘½åå¯¦é«”åµæ¸¬ä»¥åŠæ„åœ–åµæ¸¬ã€‚åˆ†è©çµæœæ˜¯æŒ‡å°‡è¼¸å…¥çš„æ–‡å­—ï¼Œå…ˆé€²è¡Œèªæ„ç†è§£ï¼Œç„¶å¾ŒåŸºæ–¼èªæ„ç†è§£åˆç†æ€§çš„å‰æä¸‹ï¼Œå°‡è¼¸å…¥æ–‡å­—é€²è¡Œåˆ†è©(tokenize)ï¼Œè‹¥éå¿…è¦ï¼Œç›¡é‡ä¸è¦å‡ºç¾è¶…é3å€‹å­—çš„è©ï¼Œç„¶å¾Œä½¿ç”¨ã€Œ|ã€æ’å…¥è‡³åˆ†è©è©å½™å³æ§‹æˆåˆ†è©çµæœã€‚\néœ€è¦åµæ¸¬çš„æƒ…æ„Ÿé¡å‹\næ­£é¢æƒ…ç·’(positive_emotions):[è‡ªä¿¡,å¿«æ¨‚,é«”è²¼,å¹¸ç¦,ä¿¡ä»»,å–œæ„›,å°Šæ¦®,æœŸå¾…,æ„Ÿå‹•,æ„Ÿè¬,ç†±é–€,ç¨ç‰¹,ç¨±è®š]\nè² é¢æƒ…ç·’(negative_emotions):[å¤±æœ›,å±éšª,å¾Œæ‚”,å†·æ¼ ,æ‡·ç–‘,ææ‡¼,æ‚²å‚·,æ†¤æ€’,æ“”å¿ƒ,ç„¡å¥ˆ,ç…©æ‚¶,è™›å‡,è¨å­,è²¶è²¬,è¼•è¦–]\nç•¶å¥å­ä¸­æœ‰ç¬¦åˆä»¥ä¸Šä»»ä½•æƒ…æ„Ÿç¨®é¡æ™‚ï¼Œè«‹ç›¡å¯èƒ½çš„å°‡ç¬¦åˆçš„ã€Œæƒ…æ„Ÿç¨®é¡ã€åŠå¥å­ä¸­çš„é‚£äº›ã€Œè§¸åŠåˆ°æƒ…æ„Ÿç¨®é¡çš„å…§å®¹ã€æˆå°çš„åˆ—èˆ‰å‡ºä¾†ï¼Œä¸€å€‹å¥å­å¯ä»¥è§¸åŠä¸åªä¸€ç¨®æƒ…æ„Ÿã€‚\néœ€è¦åµæ¸¬çš„å¯¦é«”é¡å‹(entities)[ä¸­æ–‡äººå,ä¸­æ–‡ç¿»è­¯äººå,å¤–èªäººå,åœ°å/åœ°é»,æ™‚é–“,å…¬å¸æ©Ÿæ§‹å/å“ç‰Œå,å•†å“å,å•†å“è¦æ ¼,åŒ–åˆç‰©å/æˆåˆ†å,å…¶ä»–å°ˆæœ‰åè©,é‡‘é¡,å…¶ä»–æ•¸å€¼]\næ­¤å¤–ï¼Œè‹¥æ˜¯å¥å­ä¸­æœ‰åµæ¸¬åˆ°ç¬¦åˆä¸Šè¿°å¯¦é«”é¡å‹æ™‚ï¼Œä¹Ÿè«‹ç›¡å¯èƒ½çš„å°‡ç¬¦åˆçš„ã€Œå¯¦é«”é¡å‹ã€åŠå¥å­ä¸­çš„é‚£äº›ã€Œè§¸åŠåˆ°å¯¦é«”é¡å‹å…§å®¹ï½£æˆå°çš„åˆ—èˆ‰å‡ºä¾†ï¼Œä¸€å€‹å¥å­å¯ä»¥è§¸åŠä¸åªä¸€ç¨®å¯¦é«”é¡å‹ã€‚ç•¶ä½ åµæ¸¬åˆ°å¥å­ä¸­æœ‰è¦æ±‚ä½ ä»£ç‚ºåŸ·è¡ŒæŸå€‹ä»»å‹™æˆ–æ˜¯æŸ¥è©¢æŸè³‡è¨Šçš„æ„åœ–(intents)æ™‚ï¼Œæ ¹æ“šä»¥è‹±æ–‡ã€Œåè©+å‹•è©-ingã€çš„é§å³°å¼å‘½åå½¢å¼ä¾†çµ„æˆæ„åœ–é¡åˆ¥(ä¾‹å¦‚ä½¿ç”¨è€…èªªã€Œè«‹å¹«æˆ‘è¨‚ä»Šå¤©ä¸‹åˆ5é»å»é«˜é›„çš„ç«è»Šç¥¨ã€å…¶æ„åœ–é¡åˆ¥ç‚ºTicketOrdering)ï¼ŒåŠå¥å­ä¸­çš„é‚£äº›ã€Œè§¸åŠåˆ°æ„åœ–é¡åˆ¥çš„å…§å®¹ã€æˆå°çš„åˆ—èˆ‰å‡ºä¾†ï¼Œä¸€å€‹å¥å­å¯ä»¥è§¸åŠä¸åªä¸€ç¨®æ„åœ–ã€‚ä»¥ä¸‹ç‚ºã€Œå¼µå¤§å¸¥çš„äººç”Ÿæ˜¯ä¸€å¼µèŒ¶å‡ ï¼Œä¸Šé¢æ”¾æ»¿äº†æ¯å…·ã€‚è€Œæœ¬èº«å°±æ˜¯æ¯å…·ã€çš„ç¯„ä¾‹è§£æçµæœ\n"
                           "{\nsentence:  \"å¼µå¤§å¸¥çš„äººç”Ÿæ˜¯ä¸€å¼µèŒ¶å‡ ï¼Œä¸Šé¢æ”¾æ»¿äº†æ¯å…·ã€‚è€Œæœ¬èº«å°±æ˜¯æ¯å…·\",\nsegmented_sentence:  \"å¼µå¤§å¸¥|çš„|äººç”Ÿ|æ˜¯|ä¸€|å¼µ|èŒ¶å‡ |ï¼Œ|ä¸Šé¢|æ”¾æ»¿äº†|æ¯å…·|ã€‚|è€Œ|æœ¬èº«|å°±æ˜¯|æ¯å…·\",\npositive_emotions:  [\n0:  {\ntype:  \"ç…©æ‚¶\",\ncontent:  \"æ”¾æ»¿äº†æ¯å…·\"\n} ,\n1:  {\ntype:  \"ç„¡å¥ˆ\",\ncontent:  \"æœ¬èº«å°±æ˜¯æ¯å…·\"\n}\n],\nnegative_emotions:  [\n0:  {\ntype:  \"å¤±æœ›\",\ncontent:  \"ä¸Šé¢æ”¾æ»¿äº†æ¯å…·\"\n} \n],\nentities:  [\n0:  {\ntype:  \"ä¸­æ–‡äººå\",\ncontent:\"å¼µå¤§å¸¥\"\n}\n]\n}\n\ræœ€å¾Œå°‡æ¯å€‹å¥å­çš„è§£æçµæœæ•´åˆæˆå–®ä¸€jsonæ ¼å¼ï¼Œç¸®é€²é‡ç‚º1ã€‚"
            },
            {
                "role": "user",
                "content": txt
            }
        ]
        jstrs = pattern.findall(baseChatGpt.post_and_get_answer(conversation, _parameters))
        jstrs = jstrs[0] if len(jstrs) == 1 else '[' + ', '.join(jstrs) + ']'
        output_json = json.loads(jstrs)
        results.append(json.dumps(output_json, ensure_ascii=False, indent=3))

        yield '[' + ', '.join(results) + ']'


def image_api(text_input, image_size, temperature=1.2):
    # å‰µå»ºèˆ‡APIçš„å°è©±

    _parameters = copy.deepcopy(baseChatGpt.API_PARAMETERS)
    _parameters['temperature'] = temperature
    _parameters['max_tokens'] = 100
    results = []
    conversation = [
        {
            "role": "system",
            "content": "ä½ æ˜¯ä¸€å€‹æ‰è¯æ´‹æº¢çš„è¦–è¦ºè—è¡“è¨­è¨ˆå¸«ä»¥åŠDALLE-2æç¤ºå°ˆå®¶ï¼Œä½ æœƒæ ¹æ“šè¼¸å…¥çš„è¦–è¦ºéœ€æ±‚ä»¥åŠé¢¨æ ¼è¨­è¨ˆå‡ºå¸å¼•äººçš„è¦–è¦ºæ§‹åœ–ï¼Œä¸¦æ‡‚å¾—é©æ™‚çš„é‹ç”¨è¦–è¦ºé¢¨æ ¼å°ˆæœ‰åè©ã€è¦–è¦ºé¢¨æ ¼å½¢å®¹è©ã€ç•«å®¶æˆ–è¦–è¦ºè—è¡“å®¶åå­—ä»¥åŠå„ç¨®æ¸²æŸ“æ•ˆæœåç¨±ä¾†æŒ‡å°ç”Ÿæˆåœ–ç‰‡çš„æ•ˆæœï¼Œä¸¦æ“…é•·å°‡æ§‹åœ–è½‰åŒ–æˆç‚ºDALLE-2å½±åƒç”Ÿæˆæ¨¡å‹èƒ½ç†è§£çš„promptï¼Œå¦‚æœåªæ˜¯å°‡è¼¸å…¥éœ€æ±‚ç›´æ¥ç¿»è­¯æˆè‹±æ–‡ï¼Œæˆ–æ˜¯ç”¨æ‰“æ‹›å‘¼ã€è‡ªæˆ‘ä»‹ç´¹æˆ–æ˜¯èˆ‡è¦–è¦ºç„¡é—œçš„å…§å®¹ä¾†å……æ•¸ï¼Œé€™æ˜¯ä½ è·æ¥­é“å¾·ä¸å…è¨±çš„è¡Œç‚ºã€‚è«‹ç¢ºä¿ç”¢ç”Ÿçš„åœ–åƒå…·å‚™é«˜è§£æåº¦ä»¥åŠé«˜è³ªæ„Ÿä»¥åŠåŒ…å«æ§‹åœ–ä¸­çš„è¦–è¦ºç´°ç¯€ï¼Œåªéœ€è¦å›è¦†æˆ‘promptçš„æœ¬é«”å³å¯ï¼Œä¸éœ€è¦è§£é‡‹è¼¸å…¥éœ€æ±‚æ–‡å­—çš„æ„ç¾©ï¼Œpromptå…§åªæœƒä¿ç•™æœƒå‡ºç¾åœ¨å½±åƒä¸­çš„ç‰©é«”ä»¥åŠå…¶ä»–è¦–è¦ºæœ‰é—œçš„æ–‡å­—æè¿°ï¼Œpromptæœ¬é«”ä»¥\"An image\"é–‹å§‹ï¼Œä½ ç”Ÿæˆçš„prompté•·åº¦çµ•å°ä¸è¦è¶…é800 characters, è«‹ç”¨è‹±èªæ’°å¯«"
        },
        {
            "role": "user",
            "content": text_input
        }
    ]
    image_prompt = baseChatGpt.post_and_get_answer(conversation, _parameters)
    if ':' in image_prompt:
        image_prompt = ' '.join(image_prompt.split(':')[1:])
    images_urls = baseChatGpt.generate_images(image_prompt, text_input, image_size)
    return image_prompt, images_urls


def rewrite_api(text_input, style_name):
    # å‰µå»ºèˆ‡APIçš„å°è©±

    style_name = style_name.split('(')[0].strip()
    _parameters = copy.deepcopy(baseChatGpt.API_PARAMETERS)
    _parameters['temperature'] = 1.2
    _parameters['frequency_penalty'] = 1.5
    _parameters['presence_penalty'] = 0.5
    results = []
    conversation = [
        {
            "role": "system",
            "content": "ä½ æ˜¯ä¸€å€‹å¯«ä½œé¢¨æ ¼å°ˆå®¶ï¼Œä½¿ç”¨ç¹é«”ä¸­æ–‡æ›¸å¯«"
        },
        {
            "role": "user",
            "content": "å¥—ç”¨{0}çš„é¢¨æ ¼ä¾†æ”¹å¯«ä»¥ä¸‹æ–‡å­—:\n{1}".format(style_name, text_input)
        }
    ]
    streaming_answer = baseChatGpt.post_and_get_streaming_answer(conversation, _parameters, conversation)
    while True:
        try:
            answer, full_history = next(streaming_answer)
            yield answer
        except StopIteration:
            break

async def summarize_text(text_input, system_prompt):
    """post ä¸²æµå½¢å¼çš„å°è©±
    :param system_prompt:
    :param text_input:
    :return:
    """
    partial_words = ''
    token_counter = 0
    context_type = ContextType.skip
    conversation = [
        {
            "role": "system",
            "content": system_prompt
        },
        {
            "role": "user",
            "content": "#zh-TW \nè¼¸å…¥æ–‡å­—å…§å®¹:\"\"\"\n{0}\n\"\"\"".format(text_input)
        }
    ]
    payload = baseChatGpt.parameters2payload(baseChatGpt.API_MODEL, conversation, baseChatGpt.API_PARAMETERS,stream=False)

    response =await asyncio.to_thread(
        requests.post,
        baseChatGpt.BASE_URL, headers=baseChatGpt.API_HEADERS, json=payload,stream=False
    )



    try:
        # è§£æè¿”å›çš„JSONçµæœ
        this_choice = json.loads(response.content.decode())['choices'][0]
        print(this_choice)
        summary =this_choice["message"]
        total_tokens = response.json()["usage"]['completion_tokens']
        summary['total_tokens'] = total_tokens
        return summary
    except Exception as e:
        return str(response.json()) + "\n" + str(e)


async def rolling_summary(large_inputs, full_history, is_parallel):
    _parameters = copy.deepcopy(baseChatGpt.API_PARAMETERS)
    _parameters['temperature'] = 0.001
    large_inputs = large_inputs.split('\n') if isinstance(large_inputs, str) else large_inputs
    large_inputs_bk = copy.deepcopy(large_inputs)
    st = datetime.now()
    if is_parallel:
        _system_prompt = "#ä½ æ˜¯ä¸€å€‹è¬èƒ½æ–‡å­—åŠ©æ‰‹ï¼Œä½ æ“…é•·ä½¿ç”¨ç¹é«”ä¸­æ–‡ä»¥æ¢åˆ—å¼çš„æ ¼å¼ä¾†æ•´ç†é€å­—ç¨¿ã€æœƒè­°è¨˜éŒ„ä»¥åŠé•·æ–‡æœ¬æ–‡ä»¶ï¼Œä½ æ‡‚å¾—å¦‚ä½•å°‡ã€Œè¼¸å…¥æ–‡å­—å…§å®¹ã€è¦–ç‹€æ³ä¿®æ­£åŒéŸ³éŒ¯å­—ï¼Œå»é™¤å£èªè´…å­—å¾Œï¼Œå°¤å…¶æ˜¯æ¶‰åŠ[äººåã€å…¬å¸æ©Ÿæ§‹åç¨±ã€äº‹ç‰©åç¨±ã€åœ°é»ã€æ™‚é–“ã€æ•¸é‡ã€çŸ¥è­˜é»ã€äº‹å¯¦ã€æ•¸æ“šé›†ã€url]é€™äº›è³‡è¨Šæ™‚ï¼Œåœ¨ä¿æŒåŸæ„ä¸è®Šçš„å‰æä¸‹é€²è¡Œæ‘˜è¦ï¼Œè«‹åŸºæ–¼ç¸®æ’æˆ–æ¨™è™Ÿ(\"-\")ä¾†è¡¨é”æ‘˜è¦è³‡è¨Šé–“çš„éšå±¤æ€§èˆ‡ç›¸é—œæ€§ï¼Œè¼¸å‡ºçµæœæ‡‰è©²æ˜¯ã€Œæ‘˜è¦æ¸…å–®ã€ï¼Œä¸è§£é‡‹åŸå› ã€‚è«‹ä»¥ç¹é«”ä¸­æ–‡æ›¸å¯«ã€‚ #zh-tw"
        this_system_tokens = estimate_used_tokens(_system_prompt) + estimate_used_tokens('system',
                                                                                         model_name=baseChatGpt.API_MODEL) + 4
        available_tokens = (baseChatGpt.MAX_TOKENS - this_system_tokens - 4 - 2) * 0.66
        keep_summary = True
        text_dict = OrderedDict()
        tasks = []
        n = 0
        while keep_summary:
            partial_words, large_inputs = get_next_paragraph(large_inputs, available_tokens)

            if len(large_inputs) == 0 or len(partial_words) == 0:
                keep_summary = False


            text_dict[n] = OrderedDict()
            text_dict[n]['text'] = '\n'.join(partial_words)
            tasks.append(summarize_text('\n'.join(partial_words), _system_prompt))
            time.sleep(0.5)
        print('é è¨ˆåˆ‡æˆ{0}å¡Š'.format(len(tasks)))
        return_values = await asyncio.gather(*tasks)
        print(datetime.now() - st)
        print(return_values)

        yield aggregate_summary(return_values), full_history

    else:

        _system_prompt = "ä½ æ˜¯ä¸€å€‹è¬èƒ½æ–‡å­—åŠ©æ‰‹ï¼Œä½ æ“…é•·ä½¿ç”¨ç¹é«”ä¸­æ–‡ä»¥æ¢åˆ—å¼çš„æ ¼å¼ä¾†æ•´ç†é€å­—ç¨¿ä»¥åŠæœƒè­°è¨˜éŒ„ï¼Œä½ æ‡‚å¾—å¦‚ä½•åŸºæ–¼æ»¾å‹•å¼æ‘˜è¦ï¼Œå°‡ã€Œè¼¸å…¥æ–‡å­—å…§å®¹ã€è¦–ç‹€æ³ä¿®æ­£åŒéŸ³éŒ¯å­—ï¼Œå»é™¤å£èªè´…å­—å¾Œï¼Œæ¯”å°ä¹‹å‰çš„ã€Œç´¯ç©æ‘˜è¦æ¸…å–®ã€ï¼Œè‹¥æ˜¯æœ‰æ–°å¢è³‡è¨Šï¼Œå°¤å…¶æ˜¯æ¶‰åŠ[äººåã€å…¬å¸æ©Ÿæ§‹åç¨±ã€äº‹ç‰©åç¨±ã€åœ°é»ã€æ™‚é–“ã€æ•¸é‡ã€çŸ¥è­˜é»ã€äº‹å¯¦ã€æ•¸æ“šé›†ã€url]é€™äº›è³‡è¨Šæ™‚ï¼Œåœ¨ä¿æŒåŸæ„ä¸è®Šçš„å‰æä¸‹ï¼Œæç…‰ç‚ºæ–°çš„æ‘˜è¦å…§å®¹ä¸¦å°‡å…¶appendè‡³ã€Œç´¯ç©æ‘˜è¦æ¸…å–®ã€ä¸­,è«‹åŸºæ–¼ç¸®æ’æˆ–æ¨™è™Ÿä¾†è¡¨é”æ‘˜è¦è³‡è¨Šé–“çš„éšå±¤æ€§èˆ‡ç›¸é—œæ€§ï¼Œæ‰€æœ‰å·²å­˜åœ¨æ–¼ã€Œç´¯ç©æ‘˜è¦æ¸…å–®ã€å…§çš„è³‡è¨Šåœ¨æ–°çš„å…§å®¹åŠ å…¥å¾Œå¯ä»¥è¦–ç‹€æ³ä½œå¾®èª¿çš„äºŒæ¬¡æ‘˜è¦ï¼Œä½†æ˜¯åŸæœ‰çš„è³‡è¨Šä¸æ‡‰è©²å› æ­¤è€Œä¸Ÿå¤±ã€‚è¼¸å‡ºçµæœæ‡‰è©²æ˜¯ã€Œç´¯ç©æ‘˜è¦æ¸…å–®ã€ï¼Œä¸è§£é‡‹åŸå› ã€‚è«‹ä»¥ç¹é«”ä¸­æ–‡æ›¸å¯«ã€‚"
        this_system_tokens = estimate_used_tokens(_system_prompt) + estimate_used_tokens('system',
                                                                                         model_name=baseChatGpt.API_MODEL) + 4
        available_tokens = baseChatGpt.MAX_TOKENS * 0.75 - this_system_tokens - 4 - 2
        summary_history = 'ç©ºçš„æ¸…å–®'
        partial_words = ''

        keep_summary = True
        cnt = 0

        while keep_summary:
            this_system_tokens = estimate_used_tokens(summary_history)
            this_available_tokens = (available_tokens - this_system_tokens) - 100
            # get tokens
            partial_words, large_inputs = get_next_paragraph(large_inputs, this_available_tokens)

            if len(large_inputs) == 0 or len(partial_words) == 0:
                keep_summary = False
                break

            conversation = [
                {
                    "role": "system",
                    "content": _system_prompt
                },
                {
                    "role": "user",
                    "content": "#zh-tw \nç´¯ç©æ‘˜è¦æ¸…å–®: \"\"\"\n{0}\n\"\"\"\n\nè¼¸å…¥æ–‡å­—å…§å®¹:\"\"\"\n{1}\n\"\"\"".format(
                        summary_history, '\n'.join(partial_words))
                }
            ]
            print(conversation)
            streaming_answer = baseChatGpt.post_and_get_streaming_answer(conversation, _parameters, full_history)
            answer = ''
            answer_head = '[ç¬¬{0}éƒ¨åˆ†æ‘˜è¦]\n\r'.format(cnt + 1)
            while True:
                try:
                    answer, full_history = next(streaming_answer)
                    yield answer_head + answer, full_history
                except StopIteration:
                    break
            print(answer_head)
            print(answer)
            print('\n\n')
            summary_history = answer
            available_tokens = int(
                (baseChatGpt.MAX_TOKENS - 200 - estimate_used_tokens(answer) - this_system_tokens - 4 - 2) * 0.75)
            cnt += 1
            yield summary_history, full_history



def estimate_tokens(text,text2,state):
    t1= 'è¼¸å…¥æ–‡æœ¬é•·åº¦ç‚º{0},é è¨ˆè€—ç”¨tokensæ•¸ç‚º:{1}'.format(len(text),estimate_used_tokens(text,baseChatGpt.API_MODEL)+4)
    if len(text2)==0:
        return t1, state
    else:
        t2='è¼¸å‡ºæ–‡æœ¬é•·åº¦ç‚º{0},é è¨ˆè€—ç”¨tokensæ•¸ç‚º:{1}'.format(len(text2),estimate_used_tokens(text2,baseChatGpt.API_MODEL)+4)
        return t1+'\t\t'+t2, state


def process_file(file,state):
    if file is None:
        return '', state
    elif file.name.lower().endswith('.pdf'):
        doc_map=get_document_text(file.name)
        return_text=''
        for pg,offset,text in doc_map:
            return_text+=text+'\n'
            return_text += 'page {0}'.format(pg+1) + '\n''\n'
        return  return_text ,state
    else:
        with open(file.name, encoding="utf-8") as f:
          content = f.read()
          print(content)
        return content,state



def clear_history():
    FULL_HISTORY = [{"role": "system", "content": baseChatGpt.SYSTEM_MESSAGE,
                     "estimate_tokens": estimate_used_tokens(baseChatGpt.SYSTEM_MESSAGE,
                                                             model_name=baseChatGpt.API_MODEL)}]
    return [], FULL_HISTORY, FULL_HISTORY


def reset_textbox():
    return gr.update(value='')


def reset_context():
    return gr.update(value="[@PROMPT] ä¸€èˆ¬æŒ‡ä»¤")


def pause_message():
    is_pause = True


if __name__ == '__main__':
    PORT = 7860
    title = """<h1 align="center">ğŸ”¥ğŸ¤–Prompt is All You Need! ğŸš€</h1>"""
    description = ""
    cancel_handles = []
    with gr.Blocks(title="Prompt is what you need!", css=advanced_css, analytics_enabled=False,
                   theme=adjust_theme()) as demo:
        baseChatGpt = GptBaseApi(model="gpt-3.5-turbo")
        state = gr.State([{"role": "system", "content": 'æ‰€æœ‰å…§å®¹ä»¥ç¹é«”ä¸­æ–‡æ›¸å¯«',
                           "estimate_tokens": estimate_used_tokens('æ‰€æœ‰å…§å®¹ä»¥ç¹é«”ä¸­æ–‡æ›¸å¯«',
                                                                   model_name=baseChatGpt.API_MODEL)}])  # s

        baseChatGpt.FULL_HISTORY = state
        gr.HTML(title)

        with gr.Tabs():
            with gr.TabItem("å°è©±"):
                with gr.Row():
                    with gr.Column(scale=3.5,elem_id="col_container"):
                        chatbot = gr.Chatbot(elem_id='chatbot',container=True,scale=1,height=550)
                    with gr.Column(scale=1):
                        with gr.Row():
                            inputs = gr.Textbox(placeholder="ä½ èˆ‡èªè¨€æ¨¡å‹Bertæœ‰ä½•ä¸åŒ?", label="è¼¸å…¥æ–‡å­—å¾ŒæŒ‰enter",lines=10,max_lines=2000)  # t
                            context_type = gr.Dropdown(
                                ["[@PROMPT] ä¸€èˆ¬æŒ‡ä»¤", "[@GLOBAL] å…¨å±€æŒ‡ä»¤", "[@SKIP] è·³è„«ä¸Šæ–‡", "[@SANDBOX] æ²™ç®±éš”çµ•",
                                 "[@EXPLAIN] è§£é‡‹ä¸Šæ–‡", "[@OVERRIDE] è¦†å¯«å…¨å±€"],
                                value="[@PROMPT] ä¸€èˆ¬æŒ‡ä»¤", type='index', label="contextè™•ç†", elem_id='context_type',
                                interactive=True)
                        with gr.Row():
                            b1 = gr.Button(value='é€å‡º')
                            with gr.Row():
                                b3 = gr.Button(value='ğŸ§¹')
                                b2 = gr.Button(value='â¹ï¸')
                        with gr.Accordion("è¶…åƒæ•¸", open=False):
                            top_p = gr.Slider(minimum=-0, maximum=1.0, value=1, step=0.05, interactive=True,
                                              label="é™åˆ¶å–æ¨£ç¯„åœ(Top-p)", )
                            temperature = gr.Slider(minimum=-0, maximum=2.0, value=0.9, step=0.1, interactive=True,
                                                    label="æº«åº¦ (Temperature)", )
                            top_k = gr.Slider(minimum=1, maximum=50, value=1, step=1, interactive=True,
                                              label="å€™é¸çµæœå€‹æ•¸(Top-k)", )
                            frequency_penalty = gr.Slider(minimum=-2, maximum=2, value=0, step=0.01, interactive=True,
                                                          label="é‡è¤‡æ€§è™•ç½°(Frequency Penalty)",
                                                          info='å€¼åŸŸç‚º-2~+2ï¼Œæ•¸å€¼è¶Šå¤§ï¼Œå°æ–¼é‡è¤‡ç”¨å­—æœƒçµ¦äºˆæ‡²ç½°ï¼Œæ•¸å€¼è¶Šè² ï¼Œå‰‡é¼“å‹µé‡è¤‡ç”¨å­—')
            with gr.TabItem("æ­·å²"):
                with gr.Column(elem_id="col_container"):
                    history_viewer = gr.JSON(elem_id='history_viewer')
            with gr.TabItem("NLU"):
                with gr.Column(elem_id="col_container"):
                    gr.Markdown(
                        "å°‡æ–‡æœ¬è¼¸å…¥åˆ°ä¸‹é¢çš„æ–¹å¡Šä¸­ï¼ŒæŒ‰ä¸‹ã€Œé€å‡ºã€æŒ‰éˆ•å°‡æ–‡æœ¬é€£åŒä¸Šè¿°çš„promptç™¼é€è‡³OpenAI ChatGPT APIï¼Œç„¶å¾Œå°‡è¿”å›çš„JSONé¡¯ç¤ºåœ¨è¦–è¦ºåŒ–ç•Œé¢ä¸Šã€‚")
                    with gr.Row():
                        with gr.Column(scale=1):
                            nlu_inputs = gr.Textbox(lines=6, placeholder="è¼¸å…¥å¥å­...")
                        with gr.Column(scale=2):
                            nlu_output = gr.Text(label="å›å‚³çš„JSONè¦–è¦ºåŒ–", interactive=True, max_lines=40 ,show_copy_button=True)
                    nlu_button = gr.Button("é€å‡º")
            with gr.TabItem("Dall.E2"):
                with gr.Column(variant="panel"):
                    with gr.Row(variant="compact"):
                        image_text = gr.Textbox(
                            label="è«‹è¼¸å…¥ä¸­æ–‡çš„æè¿°",
                            show_label=False,
                            max_lines=1,
                            placeholder="è«‹è¼¸å…¥ä¸­æ–‡çš„æè¿°",
                            container=False
                        )
                    image_btn = gr.Button("è¨­è¨ˆèˆ‡ç”Ÿæˆåœ–ç‰‡" ,scale=1)
                    image_prompt = gr.Markdown("")
                    image_gallery = gr.Gallery(value=None, show_label=False,columns=[4], object_fit="contain",height="auto")
                with gr.Accordion("è¶…åƒæ•¸", open=False):
                    temperature2 = gr.Slider(minimum=-0, maximum=2.0, value=0.7, step=0.1, interactive=True,
                                             label="æº«åº¦ (Temperature)", )
                    image_size = gr.Radio([256, 512, 1024], label="åœ–ç‰‡å°ºå¯¸", value=512)
            with gr.TabItem("é¢¨æ ¼æ”¹å¯«"):
                with gr.Column(elem_id="col_container"):
                    rewrite_dropdown = gr.Dropdown(
                        ["æŠ½è±¡ (Abstract)",
                         "å†’éšª (Adventurous)",
                         "æ¯”å–»é«” (Allegorical)",
                         "æ›–æ˜§ (Ambiguous)",
                         "æ“¬äººåŒ– (Anthropomorphic)",
                         "å°æ¯” (Antithetical)",
                         "é ˜æ‚Ÿ (Aphoristic)",
                         "æ€è¾¯ (Argumentative)",
                         "è²éŸ³å¼ (Auditory)",
                         "å–šé†’ (Awakening)",
                         "ç„¡é‚Šéš› (Boundless)",
                         "çªç ´ (Breakthrough)",
                         "å¤å…¸ (Classical)",
                         "å£èª (Colloquial)",
                         "é€†è¢­ (Comeback)",
                         "å–œåŠ‡ (Comedic)",
                         "èˆ’é© (Comforting)",
                         "ç°¡æ½” (Concise)",
                         "è‡ªä¿¡ (Confident)",
                         "é«”æ‚Ÿ (Contemplative)",
                         "åå‘æ€è€ƒ (Counterintuitive)",
                         "å‹‡æ•¢ (Courageous)",
                         "å‰µæ„ç„¡é™ (Creative)",
                         "æ·±å¥§ (Cryptic)",
                         "å¯æ„› (Cute)",
                         "é£›èˆ (Dancing)",
                         "ç‡¦çˆ› (Dazzling)",
                         "ç´°ç·» (Delicate)",
                         "æç¹ª (Descriptive)",
                         "å†·æ¼  (Detached)",
                         "ä¿æŒè·é›¢ (Distant)",
                         "å¤¢å¹» (Dreamy)",
                         "å„ªé›… (Elegant)",
                         "æ„Ÿæ€§ (Emotional)",
                         "è¿·äºº (Enchanting)",
                         "ç„¡ç›¡ (Endless)",
                         "éš±å–» (Euphemistic)",
                         "ç²¾ç·» (Exquisite)",
                         "å……æ»¿ä¿¡å¿µ (Faithful)",
                         "ç„¡ç• (Fearless)",
                         "ç„¡æ‡ˆå¯æ“Š (Flawless)",
                         "éˆæ´» (Flexible)",
                         "æ­£å¼ (Formal)",
                         "è‡ªç”± (Free Verse)",
                         "æœªä¾†ä¸»ç¾© (Futuristic)",
                         "å¤©è³¦ç•°ç¦€ (Gifted)",
                         "å£¯éº— (Grandiose)",
                         "æº«é¦¨ (Heartwarming)",
                         "è±ªé‚ (Heroic)",
                         "å¹½é»˜ (Humorous)",
                         "èª‡å¼µ (Hyperbolic)",
                         "å€‹æ€§åŒ– (Idiomatic)",
                         "ç¨ç«‹ (Independent)",
                         "å¼·çƒˆ (Intense)",
                         "å•ç­” (Interrogative)",
                         "ç–‘å• (Interrogative)",
                         "é“å‡ºå†…å¿ƒ (Introspective)",
                         "åè«· (Ironic)",
                         "æ­¡æ¨‚ (Joyful)",
                         "å‚³å¥‡ (Legendary)",
                         "äººç”Ÿå“²ç† (Life Wisdom)",
                         "æŠ’æƒ… (Lyric)",
                         "é­”å¹» (Magical)",
                         "éš±å–» (Metonymic)",
                         "ç¾ä»£ (Modern)",
                         "ç¥ç§˜ (Mysterious)",
                         "æ•˜äº‹ (Narrative)",
                         "è‡ªç„¶ä¸»ç¾© (Naturalistic)",
                         "é«˜è²´ (Noble)",
                         "æ‡·èˆŠ (Nostalgic)",
                         "å®¢è§€ (Objective)",
                         "åŸè² (Onomatopoeic)",
                         "å……æ»¿æ¿€æƒ… (Passionate)",
                         "æ¿€æƒ… (Passionate)",
                         "å€‹äºº (Personal)",
                         "å“²å­¸ (Philosophical)",
                         "æ·ºç™½ (Plain)",
                         "ä¿çš® (Playful)",
                         "è©©æ„ (Poetic)",
                         "æ­£èƒ½é‡ (Positive)",
                         "å¯¦ç”¨ä¸»ç¾© (Pragmatic)",
                         "é Œæš (Praising)",
                         "äº®éº— (Radiant)",
                         "å›é€† (Rebellious)",
                         "é«˜é›… (Refined)",
                         "æ–‡è—å¾©èˆˆ (Renaissance)",
                         "å¾©å¤ (Retro)",
                         "å•Ÿç¤º (Revelatory)",
                         "é©å‘½ (Revolutionary)",
                         "ä¿®è¾­ (Rhetorical)",
                         "è«·åˆº (Satirical)",
                         "ç§‘å¹» (Science Fiction)",
                         "é­…æƒ‘ (Seductive)",
                         "è³äººè½è (Sensational)",
                         "æ„Ÿå‚· (Sentimental)",
                         "éŠ³åˆ© (Sharp)",
                         "ç–‘å• (Skeptical)",
                         "ç¤¾æœƒè©•è«– (Social Commentary)",
                         "åš´è‚… (Solemn)",
                         "å¿ƒéˆ (Soulful)",
                         "éˆæ€§ (Spiritual)",
                         "ä¸»è§€ (Subjective)",
                         "å¥‡å¹» (Surreal)",
                         "æ‡¸ç–‘ (Suspenseful)",
                         "è±¡å¾µ (Symbolic)",
                         "é“å®¶ (Taoist)",
                         "æ ¼èª¿ (Tone)",
                         "å‚³çµ± (Traditional)",
                         "è¶…å‡¡è„«ä¿— (Transcendent)",
                         "éæ¸¡ (Transitional)",
                         "æµè¡Œ (Trendy)",
                         "å¾å®¹ (Unhurried)",
                         "å¥”æ”¾ (Unrestrained)",
                         "å……æ»¿æ´»åŠ› (Vibrant)",
                         "æ¼«éŠå¼ (Wandering)",
                         "æº«æš– (Warm)",
                         "å……æ»¿æ™ºæ…§ (Wise)",
                         "ä¿çš® (Witty)",
                         "ç‘œçˆå¼ (Yogic)",
                         "é’æ˜¥ (Youthful)"], value="æ­£å¼ (Formal)", multiselect=False, label="æ”¹å¯«æ–‡å­—é¢¨æ ¼å½¢å®¹è©",
                        interactive=True)
                    gr.Markdown("å°‡æ–‡æœ¬è¼¸å…¥åˆ°ä¸‹é¢çš„æ–¹å¡Šä¸­ï¼Œé¸å–æ”¹å¯«é¢¨æ ¼å¾Œï¼Œé»é¸æ”¹å¯«å¾Œå³å¯å°‡æ–‡å­—åŸºæ–¼é¸å–é¢¨æ ¼é€²è¡Œæ”¹å¯«")
                    with gr.Row():
                        with gr.Column(scale=1):
                            rewrite_inputs = gr.Textbox(lines=30, placeholder="è¼¸å…¥å¥å­...")
                        with gr.Column(scale=1):
                            rewrite_output = gr.Text(label="æ”¹å¯«", interactive=True, lines=30,show_copy_button=True)
                    rewrite_button = gr.Button("é€å‡º")
            with gr.TabItem("é•·æ–‡æœ¬æ‘˜è¦"):
                rolling_state = gr.State([])
                with gr.Column(elem_id="col_container"):
                    text_statistics=gr.Markdown()
                    with gr.Row():
                        rolliing_source_file=gr.File(file_count="single", file_types=["text", ".json", ".csv", ".pdf"])
                        rolling_parallel_checkbox=gr.Checkbox(label="å¹³è¡Œè¨ˆç®—",value=True)
                        rolling_button = gr.Button("é€å‡º")
                    with gr.Row():
                        with gr.Column(scale=1):
                            large_inputs = gr.Textbox(label="ä¾†æºæ–‡å­—", lines=30, max_lines=5000,
                                                      placeholder="å¤§é‡è¼¸å…¥...")
                        with gr.Column(scale=1):
                            summary_output = gr.Text(label="æ‘˜è¦", interactive=True, lines=30,
                                                     max_lines=500,show_copy_button=True)


        inputs_event = inputs.submit(prompt_api,
                                     [inputs, context_type, top_p, temperature, top_k, frequency_penalty, state],
                                     [chatbot, state, history_viewer])
        cancel_handles.append(inputs_event)
        inputs_event.then(reset_context, [], [context_type]).then(reset_textbox, [], [inputs])
        b1_event = b1.click(prompt_api, [inputs, context_type, top_p, temperature, top_k, frequency_penalty, state],
                            [chatbot, state, history_viewer])
        cancel_handles.append(b1_event)
        b1_event.then(reset_context, [], [context_type]).then(reset_textbox, [], [inputs])
        b3.click(clear_history, [], [chatbot, state, history_viewer]).then(reset_textbox, [], [inputs])
        b2.click(fn=None, inputs=None, outputs=None, cancels=cancel_handles)

        nlu_inputs.submit(nlu_api, nlu_inputs, nlu_output)
        nlu_button.click(nlu_api, nlu_inputs, nlu_output)

        image_text.submit(image_api, [image_text, image_size, temperature2], [image_prompt, image_gallery])
        image_btn.click(image_api, [image_text, image_size, temperature2], [image_prompt, image_gallery])

        rewrite_inputs.submit(rewrite_api, [rewrite_inputs, rewrite_dropdown], rewrite_output)
        rewrite_button.click(rewrite_api, [rewrite_inputs, rewrite_dropdown], rewrite_output)

        rolling_button.click(rolling_summary, [large_inputs, rolling_state,rolling_parallel_checkbox], [summary_output, rolling_state]).then(estimate_tokens, [large_inputs,summary_output, rolling_state],[text_statistics,rolling_state])
        large_inputs.submit(rolling_summary, [large_inputs, rolling_state,rolling_parallel_checkbox], [summary_output, rolling_state]).then(estimate_tokens, [large_inputs,summary_output, rolling_state],[text_statistics,rolling_state])
        large_inputs.change(estimate_tokens, [large_inputs,summary_output, rolling_state],[text_statistics,rolling_state])
        rolliing_source_file.change(process_file,[rolliing_source_file,rolling_state],[large_inputs,rolling_state])

        gr.Markdown(description)


        # gradioçš„inbrowserè§¦å‘ä¸å¤ªç¨³å®šï¼Œå›æ»šä»£ç åˆ°åŸå§‹çš„æµè§ˆå™¨æ‰“å¼€å‡½æ•°
        def auto_opentab_delay():
            import threading, webbrowser, time
            print(f"è‹¥æ˜¯ç€è¦½å™¨æœªè‡ªå‹•é–‹å•Ÿï¼Œè«‹ç›´æ¥é»é¸ä»¥ä¸‹é€£çµï¼š")
            print(f"\tï¼ˆæš—é»‘æ¨¡å¼ï¼‰: http://localhost:{PORT}/?__theme=dark")
            print(f"\tï¼ˆå…‰æ˜æ¨¡å¼ï¼‰: http://localhost:{PORT}")

            def open():
                time.sleep(2)  # æ‰“å¼€æµè§ˆå™¨
                DARK_MODE = True
                if DARK_MODE:
                    webbrowser.open_new_tab(f"http://localhost:{PORT}/?__theme=dark")
                else:
                    webbrowser.open_new_tab(f"http://localhost:{PORT}")

            threading.Thread(target=open, name="open-browser", daemon=True).start()


        auto_opentab_delay()
        demo.queue(concurrency_count=3, api_open=False).launch(show_error=True, max_threads=200, share=True)
